{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "situation_list = ['apology','request','thanksgiving']\n",
    "sen_type_list = ['query','res']\n",
    "src_type = 'translated' #'translated'\n",
    "ver_name = '600_culturize_all_both_lenpenalty20_direct'\n",
    "save_dir = f'data/{ver_name}/'\n",
    "data_dir = f'data/{ver_name}/'\n",
    "label_orientation = 'direct'\n",
    "intense_orientation = 'direct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_as_list(path):\n",
    "    data = []\n",
    "    with open(path, 'r', encoding='utf-8-sig')as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            data.append(row[0])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datadf(situation_list,sen_type_list,src_type):\n",
    "    df = pd.DataFrame(columns=[\"input_text\", \"target_text\"])\n",
    "    for situation in situation_list:\n",
    "        for sen_type in sen_type_list:\n",
    "            for corpus in ['mpdd','cejc']:\n",
    "                if src_type == 'original': \n",
    "                    src_path = f'/nfs/nas-7.1/yamashita/LAB/giza-pp/data/{corpus}/{situation}/{src_type}_{sen_type}.csv'\n",
    "                elif src_type == 'translated':     \n",
    "                    src_path = f'/nfs/nas-7.1/yamashita/LAB/giza-pp/data/{corpus}/{situation}/{src_type}_{sen_type}.csv'     \n",
    "                tgt_path = f'/nfs/nas-7.1/yamashita/LAB/giza-pp/data/{corpus}/{situation}/rewrited_{sen_type}.csv'\n",
    "                \n",
    "                src_data = get_data_as_list(src_path)\n",
    "                tgt_data = get_data_as_list(tgt_path)\n",
    "                \n",
    "                tmp_df = pd.DataFrame([src_data,tgt_data],index=['input_text','target_text'],columns=[src_path[40:]]*len(src_data))\n",
    "                tmp_df = tmp_df.T\n",
    "                \n",
    "                tmp_df['prefix'] = f'{situation} {sen_type}'\n",
    "                \n",
    "                df = pd.concat([df,tmp_df])\n",
    "    df = df.reset_index().set_axis(['fname','input_text','target_text','prefix'],axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tgt_list = ['ja','zh']\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "data_df = get_datadf(situation_list,sen_type_list,src_type)\n",
    "\n",
    "pureidx = np.arange(len(data_df))\n",
    "val_idx = pureidx[5::10]\n",
    "test_idx = pureidx[::10]\n",
    "\n",
    "ind = np.ones(len(data_df), dtype=bool)\n",
    "ind[val_idx] = False\n",
    "ind[test_idx] = False\n",
    "train_idx = pureidx[ind]\n",
    "# print(len(data_df))\n",
    "# print(train_idx.shape)\n",
    "# print(test_idx.shape)\n",
    "# print(val_idx.shape)\n",
    "\n",
    "train_df = data_df.iloc[train_idx]\n",
    "val_df = data_df.iloc[val_idx]\n",
    "test_df = data_df.iloc[test_idx]\n",
    "\n",
    "train_df.to_csv(save_dir+'train.csv', index=None, encoding='utf_8_sig')\n",
    "val_df.to_csv(save_dir+'val.csv', index=None, encoding='utf_8_sig')\n",
    "test_df.to_csv(save_dir+'test.csv', index=None, encoding='utf_8_sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import pandas as pd\n",
    "# from simpletransformers.t5 import T5Model, T5Args\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# transformers_logger = logging.getLogger(\"transformers\")\n",
    "# transformers_logger.setLevel(logging.WARNING)\n",
    "# # \n",
    "# data_dir = f'data/{ver_name}/'\n",
    "# train_df = pd.read_csv(f\"{data_dir}train.csv\").astype(str)\n",
    "# eval_df = pd.read_csv(f\"{data_dir}val.csv\").astype(str)\n",
    "# # train_df[\"prefix\"] = \"\"\n",
    "# # eval_df[\"prefix\"] = \"\"\n",
    "# train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_args = T5Args()\n",
    "# model_args.length_penalty = 20\n",
    "# model_args.max_seq_length = 256\n",
    "# model_args.train_batch_size = 4\n",
    "# model_args.eval_batch_size = 4\n",
    "# model_args.num_train_epochs = 20\n",
    "# model_args.evaluate_during_training = True\n",
    "# model_args.evaluate_during_training_steps = 500\n",
    "# model_args.use_multiprocessing = False\n",
    "# model_args.fp16 = False\n",
    "# model_args.early_stopping_metric = 'eval_loss'\n",
    "# model_args.early_stopping_metric_minimize = True\n",
    "# model_args.early_stopping_patience = 3\n",
    "# model_args.use_early_stopping = True\n",
    "# model_args.save_eval_checkpoints = True\n",
    "# model_args.save_eval_checkpoints = False\n",
    "# model_args.learning_rate = 3e-5\n",
    "# model_args.best_model_dir = f'outputs/{ver_name}/best_model/'\n",
    "# model_args.output_dir = f'outputs/{ver_name}/ckpt/'\n",
    "# model_args.save_model_every_epoch = True\n",
    "# model_args.save_steps = -1\n",
    "# model_args.no_cache = True\n",
    "# model_args.reprocess_input_data = True\n",
    "# model_args.overwrite_output_dir = True\n",
    "# model_args.preprocess_inputs = False\n",
    "# model_args.num_return_sequences = 1\n",
    "# model_args.wandb_project = ver_name\n",
    "\n",
    "# model = T5Model(\"mt5\", \"google/mt5-base\", args=model_args, cuda_device=1)\n",
    "# # Train the model\n",
    "# os.environ['WANDB_CONSOLE'] = 'off'\n",
    "# model.train_model(train_df[['prefix','input_text','target_text']], eval_data=eval_df[['prefix','input_text','target_text']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune with culturize label prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>prefix</th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cejc/request/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>é€™æ¨£çš„è©±ï¼Œ+å¦‚æœæˆ‘ä¸åœ¨ç¾å ´ï¼Œè€€ä¸–è³£äº†ï¼Œ+ä¹Ÿè¨±æˆ‘å¯ä»¥çµ¦è€€ä¸–ä¸€äº›ä¿è­‰é‡‘ã€‚</td>\n",
       "      <td>å¦‚æœçœŸçš„è¦æŠŠå·¥ä½œäº¤çµ¦è€€è¥¿çš„è©±...èƒ½ä¸èƒ½çµ¦ä»–å¥½ä¸€é»çš„åˆ©æ½¤å•Šï¼Ÿ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cejc/request/translated_query.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>æ˜¯çš„ï¼Œæˆ‘çŸ¥é“ã€‚é‚„æœ‰å¥¶é…ªæ£’ï¼Œè¬è¬ã€‚</td>\n",
       "      <td>å¥½ã€‚é‚£æˆ‘è¦é»ä¸€ä»½ç‚¸èµ·å¸æ¢ã€‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cejc/thanksgiving/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>æ˜¯çš„ï¼Œå…ˆç”Ÿã€‚ã€‚è¬è¬ä½ ã€‚ã€‚æ˜¯çš„ï¼Œæˆ‘çŸ¥é“ã€‚å°ä¸èµ·ï¼Œæˆ‘ä¸çŸ¥é“ã€‚è¬è¬ä½ ã€‚ã€‚å¥½å§ï¼Œé‚£å°±+ä»Šå¤©é€™æ¬¾æ‰“ä¹æŠ˜...</td>\n",
       "      <td>å¥½çš„ã€‚é€™è£¡ç‚ºæ‚¨çµå¸³ã€‚ä»Šå¤©æ‰“9æŠ˜ä¹‹å¾Œç¸½å…±æ˜¯800å…ƒã€‚é€™è£¡æ”¶æ‚¨1000å…ƒï¼Œè«‹å•æ‚¨æœ‰æœ¬åº—çš„é›†é»å¡å—ï¼Ÿ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cejc/thanksgiving/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>æˆ‘æ˜ç™½äº†ã€‚å¥½çš„ï¼Œå…ˆç”Ÿã€‚ã€‚é‚£éº¼éœ€è¦å¤šé•·æ™‚é–“å‘¢ï¼Ÿã€‚å†’éšªèª²ç¨‹å’Œå¤©å¹•èª²ç¨‹ã€‚ã€‚æ˜¯çš„ï¼Œæˆ‘çŸ¥é“ã€‚å•Šã€‚ã€‚å¥½å§...</td>\n",
       "      <td>åŸä¾†å¦‚æ­¤ï¼Œæˆ‘çŸ¥é“äº†ã€‚é‚£è«‹å•ä¸€ä¸‹æ£®æ—æ¢éšªè¡Œç¨‹å’Œéœ²ç‡Ÿè¡Œç¨‹å·®ä¸å¤šæœƒèŠ±å¤šå°‘æ™‚é–“å‘¢ï¼Ÿå¥½çš„ï¼Œå•Š...é€™æ¨£...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mpdd/apology/translated_query.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>ã”ã‚ã‚“ã­ï¼ ã“ã“æ•°æ—¥ã€å®¶ã§ã¯è‰²ã€…ã‚ã£ãŸã‚“ã§ã™ãŒ ä¼ãˆãŸã‹ã£ãŸã®ã§ã™ãŒã€å®¶åº­ã®äº‹æƒ…ã§å¿˜ã‚Œã¦ã—ã¾...</td>\n",
       "      <td>ã”ã‚ã‚“ã­ã€‚æœ€è¿‘å¿™ã—ãã¦ã€‚æœ¬å½“ã¯è¨€ã„ãŸã‹ã£ãŸã‚“ã ã‘ã©ã€æ‰‹ãŒå›ã‚‰ãªã‹ã£ãŸã®ã€‚ä»Šæ—¥å¸°ã£ãŸã‚‰çµ¶å¯¾è¨€ã†ã‹ã‚‰ã€‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>cejc/apology/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>å•Šã€‚ã€‚å°ä¸èµ·ï¼Œæˆ‘ä¸çŸ¥é“ã€‚è¬è¬ä½ ã€‚</td>\n",
       "      <td>é˜¿ï¼Œä¸å¥½æ„æ€ï¼Œéº»ç…©ä½ äº†ã€‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>cejc/apology/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>å•Šã€‚ã€‚æ˜¯çš„ï¼Œæˆ‘çŸ¥é“ã€‚æ“¦ã€‚ã€‚å°ä¸èµ·ï¼Œæˆ‘ä¸çŸ¥é“ã€‚è¬è¬ä½ ã€‚ã€‚é€™å·²ç¶“æ˜¯äº‹å¯¦äº†ï¼Œä¸æ˜¯å—ï¼Ÿã€‚æ‰€ä»¥èªªï¼Œé«’é»...</td>\n",
       "      <td>å•Šï¼Œä¸å¥½æ„æ€éº»ç…©ä½ äº†ã€‚è®“æˆ‘æ“¦ä¸€ä¸‹ã€‚é›–ç„¶èªªå¯¦åœ¨çš„é«’é«’çš„ä¹Ÿæ²’é—œä¿‚å•¦...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>cejc/apology/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>And+the+kids+mikoshi+came+out+well+the+first+b...</td>\n",
       "      <td>èª’ï¼Œå°æœ‹å‹é–‹å§‹æ‰›ç¥è½çš„æ™‚å€™ï¼Œèª’...ä¸€é–‹å§‹æ˜¯è¼ªåˆ°çŸ³äº•ä¼‘æ¯ã€‚å°æœ‹å‹å¾€å‰ä¸€é»ä¹‹å¾Œå°±æ›èˆç…ä¸Šå ´ã€‚é€™...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>cejc/request/translated_query.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>æˆ‘æƒ³è®“ä½ ç¾åœ¨å°±åƒã€‚</td>\n",
       "      <td>å¯æ˜¯ä½ ä¸ç¾åœ¨åƒçš„è©±ï¼Œå°±ä¸å¥½åƒäº†...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>cejc/request/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>è€Œä¸”+æˆ‘ä¸åœ¨å…¬äº¤è»Šä¸Šè«‡è«–æˆ‘çš„æ•™æˆã€‚</td>\n",
       "      <td>é‚„æœ‰å•Šï¼Œä¹Ÿåƒè¬ä¸è¦å†æ­å…¬è»Šçš„æ™‚å€™èªªæœ‰é—œæ•™æˆçš„è©±é¡Œã€‚</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      fname      prefix  \\\n",
       "0         cejc/request/translated_query.csv  moredirect   \n",
       "1         cejc/request/translated_query.csv  lessdirect   \n",
       "2    cejc/thanksgiving/translated_query.csv  moredirect   \n",
       "3    cejc/thanksgiving/translated_query.csv  moredirect   \n",
       "4         mpdd/apology/translated_query.csv  lessdirect   \n",
       "..                                      ...         ...   \n",
       "103       cejc/apology/translated_query.csv  moredirect   \n",
       "104       cejc/apology/translated_query.csv  moredirect   \n",
       "105       cejc/apology/translated_query.csv  moredirect   \n",
       "106       cejc/request/translated_query.csv  lessdirect   \n",
       "107       cejc/request/translated_query.csv  moredirect   \n",
       "\n",
       "                                            input_text  \\\n",
       "0                   é€™æ¨£çš„è©±ï¼Œ+å¦‚æœæˆ‘ä¸åœ¨ç¾å ´ï¼Œè€€ä¸–è³£äº†ï¼Œ+ä¹Ÿè¨±æˆ‘å¯ä»¥çµ¦è€€ä¸–ä¸€äº›ä¿è­‰é‡‘ã€‚   \n",
       "1                                     æ˜¯çš„ï¼Œæˆ‘çŸ¥é“ã€‚é‚„æœ‰å¥¶é…ªæ£’ï¼Œè¬è¬ã€‚   \n",
       "2    æ˜¯çš„ï¼Œå…ˆç”Ÿã€‚ã€‚è¬è¬ä½ ã€‚ã€‚æ˜¯çš„ï¼Œæˆ‘çŸ¥é“ã€‚å°ä¸èµ·ï¼Œæˆ‘ä¸çŸ¥é“ã€‚è¬è¬ä½ ã€‚ã€‚å¥½å§ï¼Œé‚£å°±+ä»Šå¤©é€™æ¬¾æ‰“ä¹æŠ˜...   \n",
       "3    æˆ‘æ˜ç™½äº†ã€‚å¥½çš„ï¼Œå…ˆç”Ÿã€‚ã€‚é‚£éº¼éœ€è¦å¤šé•·æ™‚é–“å‘¢ï¼Ÿã€‚å†’éšªèª²ç¨‹å’Œå¤©å¹•èª²ç¨‹ã€‚ã€‚æ˜¯çš„ï¼Œæˆ‘çŸ¥é“ã€‚å•Šã€‚ã€‚å¥½å§...   \n",
       "4    ã”ã‚ã‚“ã­ï¼ ã“ã“æ•°æ—¥ã€å®¶ã§ã¯è‰²ã€…ã‚ã£ãŸã‚“ã§ã™ãŒ ä¼ãˆãŸã‹ã£ãŸã®ã§ã™ãŒã€å®¶åº­ã®äº‹æƒ…ã§å¿˜ã‚Œã¦ã—ã¾...   \n",
       "..                                                 ...   \n",
       "103                                   å•Šã€‚ã€‚å°ä¸èµ·ï¼Œæˆ‘ä¸çŸ¥é“ã€‚è¬è¬ä½ ã€‚   \n",
       "104  å•Šã€‚ã€‚æ˜¯çš„ï¼Œæˆ‘çŸ¥é“ã€‚æ“¦ã€‚ã€‚å°ä¸èµ·ï¼Œæˆ‘ä¸çŸ¥é“ã€‚è¬è¬ä½ ã€‚ã€‚é€™å·²ç¶“æ˜¯äº‹å¯¦äº†ï¼Œä¸æ˜¯å—ï¼Ÿã€‚æ‰€ä»¥èªªï¼Œé«’é»...   \n",
       "105  And+the+kids+mikoshi+came+out+well+the+first+b...   \n",
       "106                                          æˆ‘æƒ³è®“ä½ ç¾åœ¨å°±åƒã€‚   \n",
       "107                                  è€Œä¸”+æˆ‘ä¸åœ¨å…¬äº¤è»Šä¸Šè«‡è«–æˆ‘çš„æ•™æˆã€‚   \n",
       "\n",
       "                                           target_text  \n",
       "0                       å¦‚æœçœŸçš„è¦æŠŠå·¥ä½œäº¤çµ¦è€€è¥¿çš„è©±...èƒ½ä¸èƒ½çµ¦ä»–å¥½ä¸€é»çš„åˆ©æ½¤å•Šï¼Ÿ  \n",
       "1                                        å¥½ã€‚é‚£æˆ‘è¦é»ä¸€ä»½ç‚¸èµ·å¸æ¢ã€‚  \n",
       "2      å¥½çš„ã€‚é€™è£¡ç‚ºæ‚¨çµå¸³ã€‚ä»Šå¤©æ‰“9æŠ˜ä¹‹å¾Œç¸½å…±æ˜¯800å…ƒã€‚é€™è£¡æ”¶æ‚¨1000å…ƒï¼Œè«‹å•æ‚¨æœ‰æœ¬åº—çš„é›†é»å¡å—ï¼Ÿ  \n",
       "3    åŸä¾†å¦‚æ­¤ï¼Œæˆ‘çŸ¥é“äº†ã€‚é‚£è«‹å•ä¸€ä¸‹æ£®æ—æ¢éšªè¡Œç¨‹å’Œéœ²ç‡Ÿè¡Œç¨‹å·®ä¸å¤šæœƒèŠ±å¤šå°‘æ™‚é–“å‘¢ï¼Ÿå¥½çš„ï¼Œå•Š...é€™æ¨£...  \n",
       "4    ã”ã‚ã‚“ã­ã€‚æœ€è¿‘å¿™ã—ãã¦ã€‚æœ¬å½“ã¯è¨€ã„ãŸã‹ã£ãŸã‚“ã ã‘ã©ã€æ‰‹ãŒå›ã‚‰ãªã‹ã£ãŸã®ã€‚ä»Šæ—¥å¸°ã£ãŸã‚‰çµ¶å¯¾è¨€ã†ã‹ã‚‰ã€‚  \n",
       "..                                                 ...  \n",
       "103                                       é˜¿ï¼Œä¸å¥½æ„æ€ï¼Œéº»ç…©ä½ äº†ã€‚  \n",
       "104                 å•Šï¼Œä¸å¥½æ„æ€éº»ç…©ä½ äº†ã€‚è®“æˆ‘æ“¦ä¸€ä¸‹ã€‚é›–ç„¶èªªå¯¦åœ¨çš„é«’é«’çš„ä¹Ÿæ²’é—œä¿‚å•¦...  \n",
       "105  èª’ï¼Œå°æœ‹å‹é–‹å§‹æ‰›ç¥è½çš„æ™‚å€™ï¼Œèª’...ä¸€é–‹å§‹æ˜¯è¼ªåˆ°çŸ³äº•ä¼‘æ¯ã€‚å°æœ‹å‹å¾€å‰ä¸€é»ä¹‹å¾Œå°±æ›èˆç…ä¸Šå ´ã€‚é€™...  \n",
       "106                                 å¯æ˜¯ä½ ä¸ç¾åœ¨åƒçš„è©±ï¼Œå°±ä¸å¥½åƒäº†...  \n",
       "107                          é‚„æœ‰å•Šï¼Œä¹Ÿåƒè¬ä¸è¦å†æ­å…¬è»Šçš„æ™‚å€™èªªæœ‰é—œæ•™æˆçš„è©±é¡Œã€‚  \n",
       "\n",
       "[108 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labeled_table_paths = ['JIWC_diff_reason_table.csv', 'CLIWC_diff_reason_table.csv']\n",
    "\n",
    "# label_orientation_list = [\"direct\",\"intense\",\"intense\",\"intense\",\"perspective\"]\n",
    "# intense_orientation_list =['','all','downgrader','specific','']\n",
    "\n",
    "ja_sig_list=[   ['del','cejc','query','request','Trust'],\n",
    "                ['del','cejc','query','thanksgiving','Trust'],\n",
    "                ['del','cejc','res','request','Trust'],\n",
    "                ['add','mpdd','query','apology','Disgust'],\n",
    "                ['add','mpdd','query','request','Sadness'], \n",
    "                ['add','mpdd','query','request','Disgust'],\n",
    "                ['add','mpdd','query','request','Joy'],\n",
    "                ['add','mpdd','query','thanksgiving','Sadness'],\n",
    "                ['add','mpdd','query','thanksgiving','Disgust'],\n",
    "                ['add','mpdd','query','thanksgiving','Trust'], \n",
    "                ['add','mpdd','query','thanksgiving','Joy'],\n",
    "                ['add','mpdd','res','request','Sadness'],\n",
    "                ['add','mpdd','res','request','Disgust'],\n",
    "                ['add','mpdd','res','request','Trust'],\n",
    "                ['add','mpdd','res','request','Joy'],\n",
    "                ['add','mpdd','res','thanksgiving','Sadness']]\n",
    "zh_sig_list=[   ['del',\t'mpdd',\t'query',\t'request',\t\t'affect'],\n",
    "                ['del',\t'mpdd',\t'query',\t'request',\t\t'negemo'],\n",
    "                ['del',\t'mpdd',\t'query',\t'request',\t\t'anger'],\n",
    "                ['del',\t'mpdd',\t'res',\t'thanksgiving',\t'affect'],\n",
    "                ['add',\t'cejc',\t'query',\t'apology',\t    'affect'],\n",
    "                ['add',\t'cejc',\t'query',\t'apology',\t    'posemo'],\n",
    "                ['add',\t'cejc',\t'query',\t'apology',\t    'negemo'],\n",
    "                ['add',\t'cejc',\t'query',\t'apology',\t    'anger'],\n",
    "                ['add',\t'cejc',\t'query',\t'request',\t    'negemo'],\n",
    "                ['add',\t'cejc',\t'res',\t'request',\t    'affect'],\n",
    "                ['add',\t'cejc',\t'res',\t'request',\t    'posemo']]\n",
    "\n",
    "MT_data_list,HT_data_list,prefix_list,columns_list = [],[],[],[]\n",
    "for labeled_table_path in labeled_table_paths:\n",
    "    columns_name=['diff_type','corpus','situation','sen_type','emotion','word','htmt','line','part','effect','direct','intense','perspective']\n",
    "    df = pd.read_csv(f'/nfs/nas-7.1/yamashita/LAB/giza-pp/sentiment_analysis/{labeled_table_path}', names=columns_name)\n",
    "    \n",
    "    if label_orientation == \"intense\" and intense_orientation == \"all\":\n",
    "        more =   ['lessdowngrader','moreupgrader','morespecific','lessrespectful','lesshumble','add_expect_sth_in_return','add_irony']\n",
    "        less = ['moredowngrader','lessspecific','lessupgrader','morerespectful','morehumble','rmv_expect_sth_in_return','rmv_irony']\n",
    "        for m, l in zip(more, less):\n",
    "            df=df.replace(m,'moreintense')\n",
    "            df=df.replace(l,'lessintense')\n",
    "    elif label_orientation == \"intense\":\n",
    "        pass\n",
    "    \n",
    "    if labeled_table_path == 'JIWC_diff_reason_table.csv':\n",
    "        sig_list = ja_sig_list\n",
    "    else:\n",
    "        sig_list = zh_sig_list\n",
    "        \n",
    "    for s in sig_list:\n",
    "        diff_type=s[0]\n",
    "        corpus=s[1]\n",
    "        sen_type=s[2]\n",
    "        situation=s[3]\n",
    "        emotion=s[4]\n",
    "        # FILTER TABLE\n",
    "        df = df.dropna(subset=[label_orientation])\n",
    "        emo_cond = df['diff_type'].isin([diff_type]) & df['corpus'].isin([corpus]) & df['sen_type'].isin([sen_type]) & df['situation'].isin([situation]) & df['emotion'].isin([emotion]) & df['htmt'].isin(['HT'])\n",
    "        gizamiss_cond = df['part'].isin(['gizamiss','labelmiss'])\n",
    "        line_list = df[emo_cond&~gizamiss_cond]['line'].to_list()\n",
    "        label_list = df[emo_cond&~gizamiss_cond][label_orientation].to_list()\n",
    "#         print(label_list)\n",
    "        # GET DATA\n",
    "        MT_path = f'/nfs/nas-7.1/yamashita/LAB/giza-pp/data/{corpus}/{situation}/translated_{sen_type}.csv'\n",
    "        HT_path = f'/nfs/nas-7.1/yamashita/LAB/giza-pp/data/{corpus}/{situation}/rewrited_{sen_type}.csv'\n",
    "        MT_data = get_data_as_list(MT_path)\n",
    "        HT_data = get_data_as_list(HT_path)\n",
    "\n",
    "        for line,label in zip(line_list,label_list):\n",
    "            MT_data_list.append(MT_data[line])\n",
    "            HT_data_list.append(HT_data[line])\n",
    "            prefix_list.append(label)\n",
    "            columns_list.append(MT_path[40:])\n",
    "# print(MT_data_list)\n",
    "tmp_df = pd.DataFrame([prefix_list,MT_data_list,HT_data_list],index=['prefix','input_text','target_text'],columns=columns_list)\n",
    "tmp_df = tmp_df.T\n",
    "data_df = tmp_df.drop_duplicates() \n",
    "data_df = data_df.reset_index().set_axis(['fname','prefix','input_text','target_text',],axis=1)\n",
    "display(data_df)\n",
    "pureidx = np.arange(len(data_df))\n",
    "val_idx = pureidx[5::10]\n",
    "test_idx = pureidx[::10]\n",
    "\n",
    "ind = np.ones(len(data_df), dtype=bool)\n",
    "ind[val_idx] = False\n",
    "ind[test_idx] = False\n",
    "train_idx = pureidx[ind]\n",
    "# print(len(data_df))\n",
    "# print(train_idx.shape)\n",
    "# print(test_idx.shape)\n",
    "# print(val_idx.shape)\n",
    "\n",
    "train_df = data_df.iloc[train_idx]\n",
    "val_df = data_df.iloc[val_idx]\n",
    "test_df = data_df.iloc[test_idx]\n",
    "\n",
    "train_df.to_csv(save_dir+'train.csv', encoding='utf_8_sig')\n",
    "val_df.to_csv(save_dir+'val.csv', encoding='utf_8_sig')\n",
    "test_df.to_csv(save_dir+'test.csv', encoding='utf_8_sig')\n",
    "# display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fname</th>\n",
       "      <th>prefix</th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>mpdd/apology/translated_query.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>è¬ã‚‹ã®ã¯å½“ç„¶ã®ã“ã¨ã§ã™ã€‚ æ°—æŒã¡ã¯ã‚ã‹ã‚‹ã‘ã©ã€...... æ°—æŒã¡ã¯2äººã®äººäº‹ã§ã™ã€ã‚ã‹ã‚Šã¾ã™ã‹ï¼Ÿ</td>\n",
       "      <td>è¬ã‚‹ã®ã¯ç§ã®æ–¹ã ã‚ˆã€‚ãã†ã„ã†é¢¨ã«æ€ã‚ã¦ã‚‹ã£ã¦çŸ¥ã£ã¦ãŸã‚“ã ã‹ã‚‰ã€‚ã§ã‚‚ã€ãã†ã„ã†ã®ã¯äºŒäººã®ã“ã¨ã ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>mpdd/request/translated_query.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>ã‚¸ã‚¸ãƒ¥ãƒ³ã€ã‚‚ã†å°‘ã—é™ã‹ã«ã—ã¦ãã‚Œãªã„ã‹ï¼Ÿ æ—©æœã®ãŠå–‹ã‚Šã—ã‹èã“ãˆã¦ã“ãªã„! ã“ã‚“ãªã«é¨’ã„ã§ãŸ...</td>\n",
       "      <td>é™ã‹ã«ã—ã¦ãã‚Œã€‚æœã‹ã‚‰ã‚®ãƒ£ãƒ¼ã‚®ãƒ£ãƒ¼è¨€ã‚ãªã„ã§ãã‚Œã€‚ã“ã‚“ãªã†ã‚‹ã•ã„å¥³ã‚’å«ã«ã—ãŸã„ç”·ãŒã„ã‚‹ã¨æ€ã†ã‹ï¼Ÿ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>mpdd/request/translated_res.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>æƒ…ç†±ã®ç¬é–“ã«äººã‚’å‚·ã¤ã‘ãŸã„ã®ã‹ï¼Ÿ é¶¯ã¨é­ãŒä»Šã©ã‚Œã ã‘å‹•æºã—ã¦å«Œã‚ã‚Œã¦ã„ã‚‹ã‹çŸ¥ã£ã¦ã„ã‚‹ã®ã‹ï¼Ÿ</td>\n",
       "      <td>åˆ‡ç¾½è©°ã¾ã£ãŸã‚‰ä½•ã—ã¦ã‚‚ã„ã„ã£ã¦ã“ã¨ï¼Ÿ ã©ã‚Œã ã‘äºŒäººãŒã¤ã‚‰ã„æ€ã„ã—ãŸã‹æƒ³åƒã§ãã‚‹ï¼Ÿ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>mpdd/request/translated_res.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>çš†ã•ã‚“ã€æœ¬å½“ã«ã”ã‚ã‚“ãªã•ã„! ã‚«ãƒƒãƒ—ãƒ«ã¯ã“ã“ã®çµå©šå¼å ´ãŒç¶ºéº—ã§ãƒ­ãƒãƒ³ãƒãƒƒã‚¯ã ã¨ã¯æ€ã£ã¦ã„ã¾ã›...</td>\n",
       "      <td>ç‰¹ã«å‡ºæ¥ã®è‰¯ã„å­ãŒã€ã„ã‚ã„ã‚ã¨äº‹æƒ…ãŒã‚ã£ã¦ä»Šæ—¥ã¾ã§çµå©šå¼ã‚’æŒ™ã’ã‚‰ã‚Œãªã‹ã£ãŸã®ã§ã™ãŒâ€¦â€¦ã€‚ã¯ã„...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "      <td>mpdd/request/translated_res.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>çˆºã•ã‚“ãŒè©±ã™å¿…è¦ã¯ãªã„ã€ä¿ºã¯ãƒã‚¸ãƒ¥ãƒ³ã¨ç™½é³©ã«è©±ã—ã‹ã‘ã¦ãã‚‹ã€‚ å½¼ã‚‰ã®å®¶æ—ã¯æ—é‡å±€ã®å‡ºèº«è€…ã§ã™...</td>\n",
       "      <td>å·¥å ´é•·ã˜ã‚ƒãªãã¦ã€é¦¬è»ã€ç™½é´¿ã«è¨€ãˆã°ã„ã„ã‚ˆã€‚ã¿ã‚“ãªæ—æ¥­å±€ã®ä¸€å®¶ã ã—ã€è´ã„ã¦ã‚‚ã‚‰ãˆã‚‹ã‚“ã˜ã‚ƒãªã„ã‹ãªã€‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>55</td>\n",
       "      <td>mpdd/request/translated_query.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>å…ˆç”Ÿã®ãŠæ¯æ§˜ã€ç”Ÿå¾’ä¸€äººä¸€äººã®è¦ªã¨ã—ã¦ã€å¯¾ç­‰ã§ã‚ã‚‹ã¹ãã§ã™ã€‚ ã‚¯ãƒ©ã‚¹ã®æ¥Šè²´å¦ƒãŒåŠ‰å»¶ã‚’è¿½ã„ã‹ã‘ã¦...</td>\n",
       "      <td>ç§ãŸã¡ã¿ã‚“ãªã‚ãªãŸã®å­¦ç”Ÿã§ã™ã‚ˆã­ã€‚æ¥Šã•ã‚“ã¯åŠ‰ã•ã‚“ãŒå¥½ãã¿ãŸã„ã ã‘ã©ã€ä¸Šæ‰‹ãã„ã£ã¦ã„ãªã„ã‚‰ã—ã„...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>65</td>\n",
       "      <td>mpdd/request/translated_query.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>ä»Šæ—¥ã€ç§ã€é™³å¿—æ˜ã¯ã€ã“ã“ã«ã„ã‚‹çš†ã•ã‚“ã«è¨¼è¨€ã‚’æ±‚ã‚ã¾ã™ã€ç§ã¯ã€ç§ã®ã‚¬ãƒ¼ãƒ«ãƒ•ãƒ¬ãƒ³ãƒ‰ã§ã‚ã‚‹å¼µæ„›ã¨ã®...</td>\n",
       "      <td>èª“ã„ã®è¨€è‘‰ã‚’è´ã„ã¦ãã ã•ã„ã€‚ç§ã€é™³å­æ˜ã¯ã€æ‹äººã®å¼µæ„›ã‚’ä¸€ç”Ÿå¤§äº‹ã«ã—ã¾ã™ã€‚çµ¶å¯¾ã«å‚·ã¤ã‘ã¾ã›ã‚“ã—...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>75</td>\n",
       "      <td>mpdd/request/translated_query.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>ãã†é¡˜ã„ã¾ã—ã‚‡ã†ã€‚ ã‚‚ã¡ã‚ã‚“ã‚·ãƒ£ã‚ªãƒ‰ãƒ³ã¯ä¸‹æ‰‹ããã§ã¯ãªã„ã—ã€ã‚¹ãƒƒãƒ”ãƒ³ãªã®ã§ã€ã“ã‚“ãªã‚´ã‚¿ã‚´ã‚¿ã‚’...</td>\n",
       "      <td>ãã†ã§ã™ã­ã€‚å†¬ã•ã‚“ã¯ä»•äº‹ã‚‚ã§ãã‚‹ã—ã€æ ¹æ€§ã‚‚ã‚ã‚‹ã—ã€ã“ã®ä»•äº‹ã‚’ååˆ†ã“ãªã›ã‚‹ã§ã—ã‚‡ã†ã€‚è‰¯ã‘ã‚Œã°ã€...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>85</td>\n",
       "      <td>mpdd/thanksgiving/translated_res.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>è¶™æ–Œã€ã“ã‚“ãªã‚“ã˜ã‚ƒãªã„ã‚ˆï¼Ÿ éå»ã¯æ°´ã«æµãã† ä¿ºãŸã¡ã¯ã¾ã å‹é”ã ã‹ã‚‰ãª</td>\n",
       "      <td>æ˜”ã®ã“ã¨ã ã‚ã€‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>95</td>\n",
       "      <td>mpdd/thanksgiving/translated_res.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>ã¨ã„ã†ã‹ã€å›½ãŒå¤å‚ã«ãƒãƒ£ãƒ³ã‚¹ã‚’ä¸ãˆã¦ãã‚ŒãŸã‚“ã ã‹ã‚‰ã€ãã‚Œã‚’å¤§äº‹ã«ã—ãªã„ã¨ã„ã‘ãªã„ã‚ˆã­ã€‚ ã“ã®...</td>\n",
       "      <td>ç§ã‚‚ã‚ˆãã‚ã‹ã£ã¦ãªã„ã‘ã©ã€å›½ãŒãƒãƒ£ãƒ³ã‚¹ã‚’ãã‚ŒãŸã®ã«ã€ã‚‚ã£ãŸã„ãªã„ã¨æ€ã‚ãªã„ï¼Ÿè½ã¡ãŸã£ã¦ã„ã„ã˜...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>105</td>\n",
       "      <td>cejc/apology/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>And+the+kids+mikoshi+came+out+well+the+first+b...</td>\n",
       "      <td>èª’ï¼Œå°æœ‹å‹é–‹å§‹æ‰›ç¥è½çš„æ™‚å€™ï¼Œèª’...ä¸€é–‹å§‹æ˜¯è¼ªåˆ°çŸ³äº•ä¼‘æ¯ã€‚å°æœ‹å‹å¾€å‰ä¸€é»ä¹‹å¾Œå°±æ›èˆç…ä¸Šå ´ã€‚é€™...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                 fname      prefix  \\\n",
       "0           5     mpdd/apology/translated_query.csv  lessdirect   \n",
       "1          15     mpdd/request/translated_query.csv  lessdirect   \n",
       "2          25       mpdd/request/translated_res.csv  lessdirect   \n",
       "3          35       mpdd/request/translated_res.csv  lessdirect   \n",
       "4          45       mpdd/request/translated_res.csv  lessdirect   \n",
       "5          55     mpdd/request/translated_query.csv  lessdirect   \n",
       "6          65     mpdd/request/translated_query.csv  lessdirect   \n",
       "7          75     mpdd/request/translated_query.csv  lessdirect   \n",
       "8          85  mpdd/thanksgiving/translated_res.csv  lessdirect   \n",
       "9          95  mpdd/thanksgiving/translated_res.csv  lessdirect   \n",
       "10        105     cejc/apology/translated_query.csv  moredirect   \n",
       "\n",
       "                                           input_text  \\\n",
       "0   è¬ã‚‹ã®ã¯å½“ç„¶ã®ã“ã¨ã§ã™ã€‚ æ°—æŒã¡ã¯ã‚ã‹ã‚‹ã‘ã©ã€...... æ°—æŒã¡ã¯2äººã®äººäº‹ã§ã™ã€ã‚ã‹ã‚Šã¾ã™ã‹ï¼Ÿ   \n",
       "1   ã‚¸ã‚¸ãƒ¥ãƒ³ã€ã‚‚ã†å°‘ã—é™ã‹ã«ã—ã¦ãã‚Œãªã„ã‹ï¼Ÿ æ—©æœã®ãŠå–‹ã‚Šã—ã‹èã“ãˆã¦ã“ãªã„! ã“ã‚“ãªã«é¨’ã„ã§ãŸ...   \n",
       "2       æƒ…ç†±ã®ç¬é–“ã«äººã‚’å‚·ã¤ã‘ãŸã„ã®ã‹ï¼Ÿ é¶¯ã¨é­ãŒä»Šã©ã‚Œã ã‘å‹•æºã—ã¦å«Œã‚ã‚Œã¦ã„ã‚‹ã‹çŸ¥ã£ã¦ã„ã‚‹ã®ã‹ï¼Ÿ   \n",
       "3   çš†ã•ã‚“ã€æœ¬å½“ã«ã”ã‚ã‚“ãªã•ã„! ã‚«ãƒƒãƒ—ãƒ«ã¯ã“ã“ã®çµå©šå¼å ´ãŒç¶ºéº—ã§ãƒ­ãƒãƒ³ãƒãƒƒã‚¯ã ã¨ã¯æ€ã£ã¦ã„ã¾ã›...   \n",
       "4   çˆºã•ã‚“ãŒè©±ã™å¿…è¦ã¯ãªã„ã€ä¿ºã¯ãƒã‚¸ãƒ¥ãƒ³ã¨ç™½é³©ã«è©±ã—ã‹ã‘ã¦ãã‚‹ã€‚ å½¼ã‚‰ã®å®¶æ—ã¯æ—é‡å±€ã®å‡ºèº«è€…ã§ã™...   \n",
       "5   å…ˆç”Ÿã®ãŠæ¯æ§˜ã€ç”Ÿå¾’ä¸€äººä¸€äººã®è¦ªã¨ã—ã¦ã€å¯¾ç­‰ã§ã‚ã‚‹ã¹ãã§ã™ã€‚ ã‚¯ãƒ©ã‚¹ã®æ¥Šè²´å¦ƒãŒåŠ‰å»¶ã‚’è¿½ã„ã‹ã‘ã¦...   \n",
       "6   ä»Šæ—¥ã€ç§ã€é™³å¿—æ˜ã¯ã€ã“ã“ã«ã„ã‚‹çš†ã•ã‚“ã«è¨¼è¨€ã‚’æ±‚ã‚ã¾ã™ã€ç§ã¯ã€ç§ã®ã‚¬ãƒ¼ãƒ«ãƒ•ãƒ¬ãƒ³ãƒ‰ã§ã‚ã‚‹å¼µæ„›ã¨ã®...   \n",
       "7   ãã†é¡˜ã„ã¾ã—ã‚‡ã†ã€‚ ã‚‚ã¡ã‚ã‚“ã‚·ãƒ£ã‚ªãƒ‰ãƒ³ã¯ä¸‹æ‰‹ããã§ã¯ãªã„ã—ã€ã‚¹ãƒƒãƒ”ãƒ³ãªã®ã§ã€ã“ã‚“ãªã‚´ã‚¿ã‚´ã‚¿ã‚’...   \n",
       "8                 è¶™æ–Œã€ã“ã‚“ãªã‚“ã˜ã‚ƒãªã„ã‚ˆï¼Ÿ éå»ã¯æ°´ã«æµãã† ä¿ºãŸã¡ã¯ã¾ã å‹é”ã ã‹ã‚‰ãª   \n",
       "9   ã¨ã„ã†ã‹ã€å›½ãŒå¤å‚ã«ãƒãƒ£ãƒ³ã‚¹ã‚’ä¸ãˆã¦ãã‚ŒãŸã‚“ã ã‹ã‚‰ã€ãã‚Œã‚’å¤§äº‹ã«ã—ãªã„ã¨ã„ã‘ãªã„ã‚ˆã­ã€‚ ã“ã®...   \n",
       "10  And+the+kids+mikoshi+came+out+well+the+first+b...   \n",
       "\n",
       "                                          target_text  \n",
       "0   è¬ã‚‹ã®ã¯ç§ã®æ–¹ã ã‚ˆã€‚ãã†ã„ã†é¢¨ã«æ€ã‚ã¦ã‚‹ã£ã¦çŸ¥ã£ã¦ãŸã‚“ã ã‹ã‚‰ã€‚ã§ã‚‚ã€ãã†ã„ã†ã®ã¯äºŒäººã®ã“ã¨ã ...  \n",
       "1    é™ã‹ã«ã—ã¦ãã‚Œã€‚æœã‹ã‚‰ã‚®ãƒ£ãƒ¼ã‚®ãƒ£ãƒ¼è¨€ã‚ãªã„ã§ãã‚Œã€‚ã“ã‚“ãªã†ã‚‹ã•ã„å¥³ã‚’å«ã«ã—ãŸã„ç”·ãŒã„ã‚‹ã¨æ€ã†ã‹ï¼Ÿ  \n",
       "2            åˆ‡ç¾½è©°ã¾ã£ãŸã‚‰ä½•ã—ã¦ã‚‚ã„ã„ã£ã¦ã“ã¨ï¼Ÿ ã©ã‚Œã ã‘äºŒäººãŒã¤ã‚‰ã„æ€ã„ã—ãŸã‹æƒ³åƒã§ãã‚‹ï¼Ÿ  \n",
       "3   ç‰¹ã«å‡ºæ¥ã®è‰¯ã„å­ãŒã€ã„ã‚ã„ã‚ã¨äº‹æƒ…ãŒã‚ã£ã¦ä»Šæ—¥ã¾ã§çµå©šå¼ã‚’æŒ™ã’ã‚‰ã‚Œãªã‹ã£ãŸã®ã§ã™ãŒâ€¦â€¦ã€‚ã¯ã„...  \n",
       "4   å·¥å ´é•·ã˜ã‚ƒãªãã¦ã€é¦¬è»ã€ç™½é´¿ã«è¨€ãˆã°ã„ã„ã‚ˆã€‚ã¿ã‚“ãªæ—æ¥­å±€ã®ä¸€å®¶ã ã—ã€è´ã„ã¦ã‚‚ã‚‰ãˆã‚‹ã‚“ã˜ã‚ƒãªã„ã‹ãªã€‚  \n",
       "5   ç§ãŸã¡ã¿ã‚“ãªã‚ãªãŸã®å­¦ç”Ÿã§ã™ã‚ˆã­ã€‚æ¥Šã•ã‚“ã¯åŠ‰ã•ã‚“ãŒå¥½ãã¿ãŸã„ã ã‘ã©ã€ä¸Šæ‰‹ãã„ã£ã¦ã„ãªã„ã‚‰ã—ã„...  \n",
       "6   èª“ã„ã®è¨€è‘‰ã‚’è´ã„ã¦ãã ã•ã„ã€‚ç§ã€é™³å­æ˜ã¯ã€æ‹äººã®å¼µæ„›ã‚’ä¸€ç”Ÿå¤§äº‹ã«ã—ã¾ã™ã€‚çµ¶å¯¾ã«å‚·ã¤ã‘ã¾ã›ã‚“ã—...  \n",
       "7   ãã†ã§ã™ã­ã€‚å†¬ã•ã‚“ã¯ä»•äº‹ã‚‚ã§ãã‚‹ã—ã€æ ¹æ€§ã‚‚ã‚ã‚‹ã—ã€ã“ã®ä»•äº‹ã‚’ååˆ†ã“ãªã›ã‚‹ã§ã—ã‚‡ã†ã€‚è‰¯ã‘ã‚Œã°ã€...  \n",
       "8                                             æ˜”ã®ã“ã¨ã ã‚ã€‚  \n",
       "9   ç§ã‚‚ã‚ˆãã‚ã‹ã£ã¦ãªã„ã‘ã©ã€å›½ãŒãƒãƒ£ãƒ³ã‚¹ã‚’ãã‚ŒãŸã®ã«ã€ã‚‚ã£ãŸã„ãªã„ã¨æ€ã‚ãªã„ï¼Ÿè½ã¡ãŸã£ã¦ã„ã„ã˜...  \n",
       "10  èª’ï¼Œå°æœ‹å‹é–‹å§‹æ‰›ç¥è½çš„æ™‚å€™ï¼Œèª’...ä¸€é–‹å§‹æ˜¯è¼ªåˆ°çŸ³äº•ä¼‘æ¯ã€‚å°æœ‹å‹å¾€å‰ä¸€é»ä¹‹å¾Œå°±æ›èˆç…ä¸Šå ´ã€‚é€™...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456915b438bb49f5a401b6506fb1cec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=86.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n",
      "INFO:simpletransformers.t5.t5_model: Training started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Adafactor for T5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d4460fe59e40c08e30cb22a30ee846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=10.0, style=ProgressStyle(description_width='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_model:   Starting fine-tuning.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnatsukinateyamashita\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">trim-disco-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/natsukinateyamashita/600_culturize_all_both_lenpenalty20_direct\" target=\"_blank\">https://wandb.ai/natsukinateyamashita/600_culturize_all_both_lenpenalty20_direct</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/natsukinateyamashita/600_culturize_all_both_lenpenalty20_direct/runs/spwwjxi0\" target=\"_blank\">https://wandb.ai/natsukinateyamashita/600_culturize_all_both_lenpenalty20_direct/runs/spwwjxi0</a><br/>\n",
       "                Run data is saved locally in <code>/nfs/nas-7.1/yamashita/LAB/SimpleTransformer/wandb/run-20210705_232143-spwwjxi0</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a45e11a103f4becb28e38ec0b8ea9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 0 of 10', max=22.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/torch/nn/modules/module.py:760: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n",
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/optimization.py:557: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  exp_avg_sq_row.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243f8c09cd2f47e9a02d1aa60580e81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8231af144642bda523570d58cea69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 1 of 10', max=22.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3313dbbfd9ba4c7b9113c4c209bcd618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da424333109545e58dbb236a8dfc2529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 2 of 10', max=22.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8732151a58458cb046ed70ac2ad903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c963c278cfa47419f3b1681dc9968c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 3 of 10', max=22.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6246cb04aef7469fbee2de277019fdec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b062bd197d9344fda9de95b743c11c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 4 of 10', max=22.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532209f3e2c1474ba829c4ecb16a4852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e3361d061141fb9cc2ae17ec228b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 5 of 10', max=22.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d51cbe652c24ab18a8d171975498ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2331f956e42d401ea884e0f5464ac7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 6 of 10', max=22.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dedb5a6d039949fa8ad98b70dd099b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ceb0c9bd78434cb91fa44e8bd67815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 7 of 10', max=22.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba8e8918b5e426ebf378281b42ddaf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0d7d3e5a0e455abe212a2d5849bd13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 8 of 10', max=22.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3de137245641b781101b2420d6cd92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a99e278e654a3cace9b49e1309de52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 9 of 10', max=22.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b794cc413f42c382fa54113b3e3b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_model: Training of outputs/100_culturize_all_both_lenpenalty20/best_model/ model complete. Saved to outputs/600_culturize_all_both_lenpenalty20_direct/ckpt/.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(220,\n",
       " {'global_step': [22, 44, 66, 88, 110, 132, 154, 176, 198, 220],\n",
       "  'eval_loss': [3.103783130645752,\n",
       "   3.1018369992574057,\n",
       "   3.104524294535319,\n",
       "   3.102001428604126,\n",
       "   3.1018239657084146,\n",
       "   3.1066388289133706,\n",
       "   3.105625867843628,\n",
       "   3.107701222101847,\n",
       "   3.109158913294474,\n",
       "   3.107808748881022],\n",
       "  'train_loss': [2.792762279510498,\n",
       "   2.4923272132873535,\n",
       "   4.447074890136719,\n",
       "   1.3848614692687988,\n",
       "   3.189361572265625,\n",
       "   2.800020933151245,\n",
       "   2.674372911453247,\n",
       "   2.827849864959717,\n",
       "   2.412982225418091,\n",
       "   2.2289912700653076]})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import sacrebleu\n",
    "import pandas as pd\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "data_dir = f'data/{ver_name}/'\n",
    "train_df = pd.read_csv(f\"{data_dir}train.csv\").astype(str)\n",
    "eval_df = pd.read_csv(f\"{data_dir}val.csv\").astype(str)\n",
    "# train_df[\"prefix\"] = \"\"\n",
    "# eval_df[\"prefix\"] = \"\"\n",
    "display(eval_df)\n",
    "\n",
    "model_args = T5Args()\n",
    "model_args.length_penalty = 20\n",
    "model_args.max_seq_length = 256\n",
    "model_args.train_batch_size = 4\n",
    "model_args.eval_batch_size = 4\n",
    "model_args.num_train_epochs = 10\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_steps = 500\n",
    "model_args.use_multiprocessing = False\n",
    "model_args.fp16 = False\n",
    "model_args.early_stopping_metric = 'eval_loss'\n",
    "model_args.early_stopping_metric_minimize = True\n",
    "model_args.early_stopping_patience = 3\n",
    "model_args.use_early_stopping = True\n",
    "model_args.save_eval_checkpoints = True\n",
    "model_args.save_eval_checkpoints = False\n",
    "# model_args.learning_rate = 3e-5\n",
    "model_args.learning_rate = 3e-8\n",
    "model_args.best_model_dir = f'outputs/{ver_name}/best_model/'\n",
    "model_args.output_dir = f'outputs/{ver_name}/ckpt/'\n",
    "model_args.save_model_every_epoch = True\n",
    "model_args.save_steps = -1\n",
    "model_args.no_cache = True\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.preprocess_inputs = False\n",
    "model_args.num_return_sequences = 1\n",
    "model_args.wandb_project = ver_name\n",
    "\n",
    "model = T5Model(\"mt5\", f'outputs/100_culturize_all_both_lenpenalty20/best_model/', args=model_args, cuda_device=1)\n",
    "# Train the model\n",
    "os.environ['WANDB_CONSOLE'] = 'off'\n",
    "model.train_model(train_df[['prefix','input_text','target_text']], eval_data=eval_df[['prefix','input_text','target_text']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import sacrebleu\n",
    "import pandas as pd\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "model_args = T5Args()\n",
    "model_args.max_length = 256\n",
    "model_args.length_penalty = 1\n",
    "model_args.num_beams = 10\n",
    "\n",
    "model = T5Model(\"mt5\", f\"outputs/{ver_name}/best_model/\", args=model_args, cuda_device=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fname</th>\n",
       "      <th>prefix</th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>cejc/request/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>é€™æ¨£çš„è©±ï¼Œ+å¦‚æœæˆ‘ä¸åœ¨ç¾å ´ï¼Œè€€ä¸–è³£äº†ï¼Œ+ä¹Ÿè¨±æˆ‘å¯ä»¥çµ¦è€€ä¸–ä¸€äº›ä¿è­‰é‡‘ã€‚</td>\n",
       "      <td>å¦‚æœçœŸçš„è¦æŠŠå·¥ä½œäº¤çµ¦è€€è¥¿çš„è©±...èƒ½ä¸èƒ½çµ¦ä»–å¥½ä¸€é»çš„åˆ©æ½¤å•Šï¼Ÿ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>mpdd/request/translated_query.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>é ¼ã‚€ã‹ã‚‰çˆ¶ã®é©å‘½å®¶ã®é¡”ã®ãŸã‚ã«ã‚‚ è§£æ”¾ã—ã¦ãã‚Œã‚ˆï¼ ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™ï¼ã€ã¨è¨€ã£ã¦ã„ã¾ã—ãŸ...</td>\n",
       "      <td>çˆ¶ã®é©å‘½å®¶ã¨ã—ã¦ã®é¡”ã‚’ç«‹ã¦ã¦ã€ãŠé¡˜ã„å‡ºæ¥ã¾ã›ã‚“ã‹ã€‚ãŠé¡˜ã„ã„ãŸã—ã¾ã™ã€‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>mpdd/request/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>æè¯ã€ç¤¼ç¯€ã‚’ã‚ãã¾ãˆã¦ãã ã•ã„ã€ç®—æ•°ã®å•é¡Œã‚’è­°è«–ã—ã¦ã„ã‚‹ã‚“ã§ã™ã‚ˆ! ä½•ã‚’çŸ¥ã£ã¦ã‚‹ã‚“ã ï¼</td>\n",
       "      <td>ä»Šã‚€ãšã‹ã—ã„æ•°å­¦ã®å•é¡Œã‚’è§£ã„ã¦ã‚‹ã‚“ã ã‚ˆã€‚è¦‹ã‚Œã°ã‚ã‹ã‚‹ã ã‚ï¼Ÿ å¾Œã«ã—ã¦ãã‚Œã€‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>mpdd/request/translated_res.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>æŸ¯ã•ã‚“ã€å¤–è¦‹ã¯è»Ÿå¼±ã ã‘ã©ã€è¨€è‘‰ã®ã‚­ãƒ¬ãŒã™ã”ã„ã§ã™ã­!</td>\n",
       "      <td>ãŠè©±ã™ã‚‹ã¾ã§ã€ã“ã‚“ãªã«é‹­ã„æ–¹ã ã¨ã¯æ€ã„ã¾ã›ã‚“ã§ã—ãŸã€‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>mpdd/request/translated_res.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>ä¸»å‚¬è€…å¤‰æ›´ç”³è«‹ã®å ±å‘Šã€‚ ã“ã‚Œã¯ç®¡ç†äº‹å‹™æ‰€ã®ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚ éƒ­çˆºã«è¦‹ã›ã‚Œã°ã„ã„ã‚“ã ã‚ˆã€‚</td>\n",
       "      <td>ã€Œä¸»å‚¬è€…å¤‰æ›´ã®ç”³è«‹ãƒ¬ãƒãƒ¼ãƒˆã€ã“ã‚Œã¯ç®¡ç†æ‰€ã®ä»•äº‹ã ã‚ˆã€‚éƒ­ã•ã‚“ã«è¦‹ã›ã¦ãã‚Œã€‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>mpdd/request/translated_res.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>å›ã¯è‡ªåˆ†ã®ä»•äº‹ã‚’ã—ã¦ ç§ã¯æ€¥ã„ã§ã„ãªã„</td>\n",
       "      <td>ãŠæ°—ã«ãªã•ã‚‰ãšã€‚ç§ã¯æ€¥ãã¾ã›ã‚“ã‹ã‚‰ã€‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>60</td>\n",
       "      <td>mpdd/request/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>ã“ã‚Œã¯ã€ã‚ã¾ã‚Šä¾¿åˆ©ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ ä¸€ã¤ã«ã¯ã€æ­£åˆã«äº‹å‹™æ‰€ã§ä¼‘ã¾ãªã‘ã‚Œã°ãªã‚‰ãªã„ã“ã¨ã€ã‚‚ã†ä¸€...</td>\n",
       "      <td>ãã‚Œã¯å›°ã‚Šã¾ã™ã€‚ç§ã¯ãŠæ˜¼ã¯ã“ã“ã§ä¼‘ã¿ã¾ã™ã€‚ãã‚Œã«ã€ç”·å¥³ãŒåŒã˜ã‚ªãƒ•ã‚£ã‚¹ã«ã„ã‚‹ã®ã‚‚ã‚ã‚Œã§ã—ã‚‡ã†ã†...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>70</td>\n",
       "      <td>mpdd/request/translated_query.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>åŠ‰ã•ã‚“ã€ã‚ãªãŸã®é¸æŠã«æ•¬æ„ã‚’è¡¨ã—ã¾ã™ï¼ç§ã®å¿ƒã®ä¸­ã§ã¯ç´ æ™´ã‚‰ã—ã„å¥³æ€§ã§ã™ï¼ä»Šå¾Œã®å¹¸ã›ãªçµå©šã‚’ãŠ...</td>\n",
       "      <td>ãã†ã§ã™ã­ã€‚åƒ•ãŒé–“é•ã£ã¦ã„ã¾ã—ãŸã€‚è¨±ã—ã¦ãã ã•ã„ã€‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>80</td>\n",
       "      <td>mpdd/request/translated_query.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>èª°ãŒãšã£ã¨å¾…ã£ã¦ãŸã‚“ã ã‚ˆï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—...</td>\n",
       "      <td>ä½•è¨€ã£ã¦ã‚“ã®ï¼Ÿ å¥¥æ§˜ã«ç”¨äº‹ãŒã‚ã£ã¦ããŸã ã‘ã ã‹ã‚‰ã€èª¤è§£ã—ãªã„ã§ãã‚Œã‚‹ï¼Ÿ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>90</td>\n",
       "      <td>mpdd/thanksgiving/translated_res.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>ã‚ã®æ—¥ã®ã‚ãªãŸã¯èŒ¨ã«è¦†ã‚ã‚ŒãŸãƒãƒªãƒã‚ºãƒŸã®ã‚ˆã†ã§ã€ä»Šæ—¥ã®ã‚ãªãŸã¯ã‚ã®æ—¥ã¨ã¯åˆ¥äººã®ã‚ˆã†ã§ã™! æ…‹...</td>\n",
       "      <td>ãšã„ã¶ã‚“é›°å›²æ°—ãŒæŸ”ã‚‰ã‹ããªã‚Šã¾ã—ãŸã­ã€‚ã‚‚ã†ã²ã¨ã¤ãŠé¡˜ã„ã—ã¦ã‚‚ã‚ˆã‚ã—ã„ã§ã™ã‹ï¼Ÿ å‹äººã¨ã—ã¦ä»˜ã...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100</td>\n",
       "      <td>cejc/apology/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>é€™æ˜¯ä»€éº¼ï¼Ÿã€‚æŠ±æ­‰ï¼Œèƒ¡æ¤’çš„äº‹ã€‚</td>\n",
       "      <td>é€™æ˜¯ä»€éº¼ï¼Ÿå•Šã€æ˜¯é’æ¤’å•Š...æŠ±æ­‰ï¼Œæˆ‘ä¸æ˜¯å¾ˆå–œæ­¡åƒèª’ã€‚</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                 fname      prefix  \\\n",
       "0           0     cejc/request/translated_query.csv  moredirect   \n",
       "1          10     mpdd/request/translated_query.csv  lessdirect   \n",
       "2          20     mpdd/request/translated_query.csv  moredirect   \n",
       "3          30       mpdd/request/translated_res.csv  lessdirect   \n",
       "4          40       mpdd/request/translated_res.csv  moredirect   \n",
       "5          50       mpdd/request/translated_res.csv  lessdirect   \n",
       "6          60     mpdd/request/translated_query.csv  moredirect   \n",
       "7          70     mpdd/request/translated_query.csv  lessdirect   \n",
       "8          80     mpdd/request/translated_query.csv  lessdirect   \n",
       "9          90  mpdd/thanksgiving/translated_res.csv  lessdirect   \n",
       "10        100     cejc/apology/translated_query.csv  moredirect   \n",
       "\n",
       "                                           input_text  \\\n",
       "0                  é€™æ¨£çš„è©±ï¼Œ+å¦‚æœæˆ‘ä¸åœ¨ç¾å ´ï¼Œè€€ä¸–è³£äº†ï¼Œ+ä¹Ÿè¨±æˆ‘å¯ä»¥çµ¦è€€ä¸–ä¸€äº›ä¿è­‰é‡‘ã€‚   \n",
       "1   é ¼ã‚€ã‹ã‚‰çˆ¶ã®é©å‘½å®¶ã®é¡”ã®ãŸã‚ã«ã‚‚ è§£æ”¾ã—ã¦ãã‚Œã‚ˆï¼ ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™ï¼ã€ã¨è¨€ã£ã¦ã„ã¾ã—ãŸ...   \n",
       "2         æè¯ã€ç¤¼ç¯€ã‚’ã‚ãã¾ãˆã¦ãã ã•ã„ã€ç®—æ•°ã®å•é¡Œã‚’è­°è«–ã—ã¦ã„ã‚‹ã‚“ã§ã™ã‚ˆ! ä½•ã‚’çŸ¥ã£ã¦ã‚‹ã‚“ã ï¼   \n",
       "3                          æŸ¯ã•ã‚“ã€å¤–è¦‹ã¯è»Ÿå¼±ã ã‘ã©ã€è¨€è‘‰ã®ã‚­ãƒ¬ãŒã™ã”ã„ã§ã™ã­!   \n",
       "4         ä¸»å‚¬è€…å¤‰æ›´ç”³è«‹ã®å ±å‘Šã€‚ ã“ã‚Œã¯ç®¡ç†äº‹å‹™æ‰€ã®ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚ éƒ­çˆºã«è¦‹ã›ã‚Œã°ã„ã„ã‚“ã ã‚ˆã€‚   \n",
       "5                                 å›ã¯è‡ªåˆ†ã®ä»•äº‹ã‚’ã—ã¦ ç§ã¯æ€¥ã„ã§ã„ãªã„   \n",
       "6   ã“ã‚Œã¯ã€ã‚ã¾ã‚Šä¾¿åˆ©ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ ä¸€ã¤ã«ã¯ã€æ­£åˆã«äº‹å‹™æ‰€ã§ä¼‘ã¾ãªã‘ã‚Œã°ãªã‚‰ãªã„ã“ã¨ã€ã‚‚ã†ä¸€...   \n",
       "7   åŠ‰ã•ã‚“ã€ã‚ãªãŸã®é¸æŠã«æ•¬æ„ã‚’è¡¨ã—ã¾ã™ï¼ç§ã®å¿ƒã®ä¸­ã§ã¯ç´ æ™´ã‚‰ã—ã„å¥³æ€§ã§ã™ï¼ä»Šå¾Œã®å¹¸ã›ãªçµå©šã‚’ãŠ...   \n",
       "8   èª°ãŒãšã£ã¨å¾…ã£ã¦ãŸã‚“ã ã‚ˆï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—ï½—...   \n",
       "9   ã‚ã®æ—¥ã®ã‚ãªãŸã¯èŒ¨ã«è¦†ã‚ã‚ŒãŸãƒãƒªãƒã‚ºãƒŸã®ã‚ˆã†ã§ã€ä»Šæ—¥ã®ã‚ãªãŸã¯ã‚ã®æ—¥ã¨ã¯åˆ¥äººã®ã‚ˆã†ã§ã™! æ…‹...   \n",
       "10                                     é€™æ˜¯ä»€éº¼ï¼Ÿã€‚æŠ±æ­‰ï¼Œèƒ¡æ¤’çš„äº‹ã€‚   \n",
       "\n",
       "                                          target_text  \n",
       "0                      å¦‚æœçœŸçš„è¦æŠŠå·¥ä½œäº¤çµ¦è€€è¥¿çš„è©±...èƒ½ä¸èƒ½çµ¦ä»–å¥½ä¸€é»çš„åˆ©æ½¤å•Šï¼Ÿ  \n",
       "1                  çˆ¶ã®é©å‘½å®¶ã¨ã—ã¦ã®é¡”ã‚’ç«‹ã¦ã¦ã€ãŠé¡˜ã„å‡ºæ¥ã¾ã›ã‚“ã‹ã€‚ãŠé¡˜ã„ã„ãŸã—ã¾ã™ã€‚  \n",
       "2               ä»Šã‚€ãšã‹ã—ã„æ•°å­¦ã®å•é¡Œã‚’è§£ã„ã¦ã‚‹ã‚“ã ã‚ˆã€‚è¦‹ã‚Œã°ã‚ã‹ã‚‹ã ã‚ï¼Ÿ å¾Œã«ã—ã¦ãã‚Œã€‚  \n",
       "3                          ãŠè©±ã™ã‚‹ã¾ã§ã€ã“ã‚“ãªã«é‹­ã„æ–¹ã ã¨ã¯æ€ã„ã¾ã›ã‚“ã§ã—ãŸã€‚  \n",
       "4                ã€Œä¸»å‚¬è€…å¤‰æ›´ã®ç”³è«‹ãƒ¬ãƒãƒ¼ãƒˆã€ã“ã‚Œã¯ç®¡ç†æ‰€ã®ä»•äº‹ã ã‚ˆã€‚éƒ­ã•ã‚“ã«è¦‹ã›ã¦ãã‚Œã€‚  \n",
       "5                                  ãŠæ°—ã«ãªã•ã‚‰ãšã€‚ç§ã¯æ€¥ãã¾ã›ã‚“ã‹ã‚‰ã€‚  \n",
       "6   ãã‚Œã¯å›°ã‚Šã¾ã™ã€‚ç§ã¯ãŠæ˜¼ã¯ã“ã“ã§ä¼‘ã¿ã¾ã™ã€‚ãã‚Œã«ã€ç”·å¥³ãŒåŒã˜ã‚ªãƒ•ã‚£ã‚¹ã«ã„ã‚‹ã®ã‚‚ã‚ã‚Œã§ã—ã‚‡ã†ã†...  \n",
       "7                           ãã†ã§ã™ã­ã€‚åƒ•ãŒé–“é•ã£ã¦ã„ã¾ã—ãŸã€‚è¨±ã—ã¦ãã ã•ã„ã€‚  \n",
       "8                 ä½•è¨€ã£ã¦ã‚“ã®ï¼Ÿ å¥¥æ§˜ã«ç”¨äº‹ãŒã‚ã£ã¦ããŸã ã‘ã ã‹ã‚‰ã€èª¤è§£ã—ãªã„ã§ãã‚Œã‚‹ï¼Ÿ  \n",
       "9   ãšã„ã¶ã‚“é›°å›²æ°—ãŒæŸ”ã‚‰ã‹ããªã‚Šã¾ã—ãŸã­ã€‚ã‚‚ã†ã²ã¨ã¤ãŠé¡˜ã„ã—ã¦ã‚‚ã‚ˆã‚ã—ã„ã§ã™ã‹ï¼Ÿ å‹äººã¨ã—ã¦ä»˜ã...  \n",
       "10                         é€™æ˜¯ä»€éº¼ï¼Ÿå•Šã€æ˜¯é’æ¤’å•Š...æŠ±æ­‰ï¼Œæˆ‘ä¸æ˜¯å¾ˆå–œæ­¡åƒèª’ã€‚  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "eval_df = pd.read_csv(f\"{data_dir}test.csv\").astype(str)\n",
    "display(eval_df)\n",
    "to_ja_truth = [eval_df.loc[eval_df[\"fname\"].str.contains(\"mpdd\")][\"target_text\"].tolist()]\n",
    "to_ja_input = eval_df.loc[eval_df[\"fname\"].str.contains(\"mpdd\")][\"input_text\"].tolist()\n",
    "\n",
    "to_zh_truth = [eval_df.loc[eval_df[\"fname\"].str.contains(\"cejc\")][\"target_text\"].tolist()]\n",
    "to_zh_input = eval_df.loc[eval_df[\"fname\"].str.contains(\"cejc\")][\"input_text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6b01d4e0c74dc7b3327654c5b0ac1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Generating outputs', max=2.0, style=ProgressStyle(descripâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196150add4cd4deab0bf9b2e790e5ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Decoding outputs', max=9.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------\n",
      "to_ja_bleu:  4.826217438701122\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a92606b10d47c09c49ff14f65c649c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Generating outputs', max=1.0, style=ProgressStyle(descripâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48728e3509e744f982f4515142c71544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Decoding outputs', max=2.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------\n",
      "to_zh_bleu:  2.3040887376159365\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "to_ja_preds = model.predict(to_ja_input)\n",
    "to_ja_bleu = sacrebleu.corpus_bleu(to_ja_preds, to_ja_truth)\n",
    "print(\"--------------------------\")\n",
    "print(\"to_ja_bleu: \", to_ja_bleu.score)\n",
    "\n",
    "to_zh_preds = model.predict(to_zh_input)\n",
    "\n",
    "to_zh_bleu = sacrebleu.corpus_bleu(to_zh_preds, to_zh_truth)\n",
    "print(\"--------------------------\")\n",
    "print(\"to_zh_bleu: \", to_zh_bleu.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_ja_preds.extend(to_zh_preds)\n",
    "to_ja_truth_ = to_ja_truth[0]\n",
    "to_ja_truth_.extend(to_zh_truth[0])\n",
    "\n",
    "r_df = pd.DataFrame([to_ja_preds,to_ja_truth_],index=[f'{ver_name}_preds', 'truth'])\n",
    "r_df.T.to_csv(f'outputs/{ver_name}/preds_truth.csv',encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_df= pd.DataFrame([to_ja_bleu.score,to_zh_bleu.score], index=['to_ja_bleu.score','to_zh_bleu.score'])\n",
    "blue_df.to_csv(f'outputs/{ver_name}/bluescore.csv',encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
