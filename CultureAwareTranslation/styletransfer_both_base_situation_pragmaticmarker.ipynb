{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "situation_list = ['apology','request','thanksgiving']\n",
    "sen_type_list = ['query','res']\n",
    "src_type = 'translated' #'translated'\n",
    "ver_name = '600_culturize_all_both_lenpenalty20_direct'\n",
    "save_dir = f'data/{ver_name}/'\n",
    "data_dir = f'data/{ver_name}/'\n",
    "label_orientation = 'direct'\n",
    "intense_orientation = 'direct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_as_list(path):\n",
    "    data = []\n",
    "    with open(path, 'r', encoding='utf-8-sig')as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            data.append(row[0])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datadf(situation_list,sen_type_list,src_type):\n",
    "    df = pd.DataFrame(columns=[\"input_text\", \"target_text\"])\n",
    "    for situation in situation_list:\n",
    "        for sen_type in sen_type_list:\n",
    "            for corpus in ['mpdd','cejc']:\n",
    "                if src_type == 'original': \n",
    "                    src_path = f'/nfs/nas-7.1/yamashita/LAB/giza-pp/data/{corpus}/{situation}/{src_type}_{sen_type}.csv'\n",
    "                elif src_type == 'translated':     \n",
    "                    src_path = f'/nfs/nas-7.1/yamashita/LAB/giza-pp/data/{corpus}/{situation}/{src_type}_{sen_type}.csv'     \n",
    "                tgt_path = f'/nfs/nas-7.1/yamashita/LAB/giza-pp/data/{corpus}/{situation}/rewrited_{sen_type}.csv'\n",
    "                \n",
    "                src_data = get_data_as_list(src_path)\n",
    "                tgt_data = get_data_as_list(tgt_path)\n",
    "                \n",
    "                tmp_df = pd.DataFrame([src_data,tgt_data],index=['input_text','target_text'],columns=[src_path[40:]]*len(src_data))\n",
    "                tmp_df = tmp_df.T\n",
    "                \n",
    "                tmp_df['prefix'] = f'{situation} {sen_type}'\n",
    "                \n",
    "                df = pd.concat([df,tmp_df])\n",
    "    df = df.reset_index().set_axis(['fname','input_text','target_text','prefix'],axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tgt_list = ['ja','zh']\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "data_df = get_datadf(situation_list,sen_type_list,src_type)\n",
    "\n",
    "pureidx = np.arange(len(data_df))\n",
    "val_idx = pureidx[5::10]\n",
    "test_idx = pureidx[::10]\n",
    "\n",
    "ind = np.ones(len(data_df), dtype=bool)\n",
    "ind[val_idx] = False\n",
    "ind[test_idx] = False\n",
    "train_idx = pureidx[ind]\n",
    "# print(len(data_df))\n",
    "# print(train_idx.shape)\n",
    "# print(test_idx.shape)\n",
    "# print(val_idx.shape)\n",
    "\n",
    "train_df = data_df.iloc[train_idx]\n",
    "val_df = data_df.iloc[val_idx]\n",
    "test_df = data_df.iloc[test_idx]\n",
    "\n",
    "train_df.to_csv(save_dir+'train.csv', index=None, encoding='utf_8_sig')\n",
    "val_df.to_csv(save_dir+'val.csv', index=None, encoding='utf_8_sig')\n",
    "test_df.to_csv(save_dir+'test.csv', index=None, encoding='utf_8_sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import pandas as pd\n",
    "# from simpletransformers.t5 import T5Model, T5Args\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# transformers_logger = logging.getLogger(\"transformers\")\n",
    "# transformers_logger.setLevel(logging.WARNING)\n",
    "# # \n",
    "# data_dir = f'data/{ver_name}/'\n",
    "# train_df = pd.read_csv(f\"{data_dir}train.csv\").astype(str)\n",
    "# eval_df = pd.read_csv(f\"{data_dir}val.csv\").astype(str)\n",
    "# # train_df[\"prefix\"] = \"\"\n",
    "# # eval_df[\"prefix\"] = \"\"\n",
    "# train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_args = T5Args()\n",
    "# model_args.length_penalty = 20\n",
    "# model_args.max_seq_length = 256\n",
    "# model_args.train_batch_size = 4\n",
    "# model_args.eval_batch_size = 4\n",
    "# model_args.num_train_epochs = 20\n",
    "# model_args.evaluate_during_training = True\n",
    "# model_args.evaluate_during_training_steps = 500\n",
    "# model_args.use_multiprocessing = False\n",
    "# model_args.fp16 = False\n",
    "# model_args.early_stopping_metric = 'eval_loss'\n",
    "# model_args.early_stopping_metric_minimize = True\n",
    "# model_args.early_stopping_patience = 3\n",
    "# model_args.use_early_stopping = True\n",
    "# model_args.save_eval_checkpoints = True\n",
    "# model_args.save_eval_checkpoints = False\n",
    "# model_args.learning_rate = 3e-5\n",
    "# model_args.best_model_dir = f'outputs/{ver_name}/best_model/'\n",
    "# model_args.output_dir = f'outputs/{ver_name}/ckpt/'\n",
    "# model_args.save_model_every_epoch = True\n",
    "# model_args.save_steps = -1\n",
    "# model_args.no_cache = True\n",
    "# model_args.reprocess_input_data = True\n",
    "# model_args.overwrite_output_dir = True\n",
    "# model_args.preprocess_inputs = False\n",
    "# model_args.num_return_sequences = 1\n",
    "# model_args.wandb_project = ver_name\n",
    "\n",
    "# model = T5Model(\"mt5\", \"google/mt5-base\", args=model_args, cuda_device=1)\n",
    "# # Train the model\n",
    "# os.environ['WANDB_CONSOLE'] = 'off'\n",
    "# model.train_model(train_df[['prefix','input_text','target_text']], eval_data=eval_df[['prefix','input_text','target_text']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune with culturize label prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>prefix</th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cejc/request/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>這樣的話，+如果我不在現場，耀世賣了，+也許我可以給耀世一些保證金。</td>\n",
       "      <td>如果真的要把工作交給耀西的話...能不能給他好一點的利潤啊？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cejc/request/translated_query.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>是的，我知道。還有奶酪棒，謝謝。</td>\n",
       "      <td>好。那我要點一份炸起司條。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cejc/thanksgiving/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>是的，先生。。謝謝你。。是的，我知道。對不起，我不知道。謝謝你。。好吧，那就+今天這款打九折...</td>\n",
       "      <td>好的。這裡為您結帳。今天打9折之後總共是800元。這裡收您1000元，請問您有本店的集點卡嗎？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cejc/thanksgiving/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>我明白了。好的，先生。。那麼需要多長時間呢？。冒險課程和天幕課程。。是的，我知道。啊。。好吧...</td>\n",
       "      <td>原來如此，我知道了。那請問一下森林探險行程和露營行程差不多會花多少時間呢？好的，啊...這樣...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mpdd/apology/translated_query.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>ごめんね！ ここ数日、家では色々あったんですが 伝えたかったのですが、家庭の事情で忘れてしま...</td>\n",
       "      <td>ごめんね。最近忙しくて。本当は言いたかったんだけど、手が回らなかったの。今日帰ったら絶対言うから。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>cejc/apology/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>啊。。對不起，我不知道。謝謝你。</td>\n",
       "      <td>阿，不好意思，麻煩你了。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>cejc/apology/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>啊。。是的，我知道。擦。。對不起，我不知道。謝謝你。。這已經是事實了，不是嗎？。所以說，髒點...</td>\n",
       "      <td>啊，不好意思麻煩你了。讓我擦一下。雖然說實在的髒髒的也沒關係啦...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>cejc/apology/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>And+the+kids+mikoshi+came+out+well+the+first+b...</td>\n",
       "      <td>誒，小朋友開始扛神轎的時候，誒...一開始是輪到石井休息。小朋友往前一點之後就換舞獅上場。這...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>cejc/request/translated_query.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>我想讓你現在就吃。</td>\n",
       "      <td>可是你不現在吃的話，就不好吃了...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>cejc/request/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>而且+我不在公交車上談論我的教授。</td>\n",
       "      <td>還有啊，也千萬不要再搭公車的時候說有關教授的話題。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      fname      prefix  \\\n",
       "0         cejc/request/translated_query.csv  moredirect   \n",
       "1         cejc/request/translated_query.csv  lessdirect   \n",
       "2    cejc/thanksgiving/translated_query.csv  moredirect   \n",
       "3    cejc/thanksgiving/translated_query.csv  moredirect   \n",
       "4         mpdd/apology/translated_query.csv  lessdirect   \n",
       "..                                      ...         ...   \n",
       "103       cejc/apology/translated_query.csv  moredirect   \n",
       "104       cejc/apology/translated_query.csv  moredirect   \n",
       "105       cejc/apology/translated_query.csv  moredirect   \n",
       "106       cejc/request/translated_query.csv  lessdirect   \n",
       "107       cejc/request/translated_query.csv  moredirect   \n",
       "\n",
       "                                            input_text  \\\n",
       "0                   這樣的話，+如果我不在現場，耀世賣了，+也許我可以給耀世一些保證金。   \n",
       "1                                     是的，我知道。還有奶酪棒，謝謝。   \n",
       "2    是的，先生。。謝謝你。。是的，我知道。對不起，我不知道。謝謝你。。好吧，那就+今天這款打九折...   \n",
       "3    我明白了。好的，先生。。那麼需要多長時間呢？。冒險課程和天幕課程。。是的，我知道。啊。。好吧...   \n",
       "4    ごめんね！ ここ数日、家では色々あったんですが 伝えたかったのですが、家庭の事情で忘れてしま...   \n",
       "..                                                 ...   \n",
       "103                                   啊。。對不起，我不知道。謝謝你。   \n",
       "104  啊。。是的，我知道。擦。。對不起，我不知道。謝謝你。。這已經是事實了，不是嗎？。所以說，髒點...   \n",
       "105  And+the+kids+mikoshi+came+out+well+the+first+b...   \n",
       "106                                          我想讓你現在就吃。   \n",
       "107                                  而且+我不在公交車上談論我的教授。   \n",
       "\n",
       "                                           target_text  \n",
       "0                       如果真的要把工作交給耀西的話...能不能給他好一點的利潤啊？  \n",
       "1                                        好。那我要點一份炸起司條。  \n",
       "2      好的。這裡為您結帳。今天打9折之後總共是800元。這裡收您1000元，請問您有本店的集點卡嗎？  \n",
       "3    原來如此，我知道了。那請問一下森林探險行程和露營行程差不多會花多少時間呢？好的，啊...這樣...  \n",
       "4    ごめんね。最近忙しくて。本当は言いたかったんだけど、手が回らなかったの。今日帰ったら絶対言うから。  \n",
       "..                                                 ...  \n",
       "103                                       阿，不好意思，麻煩你了。  \n",
       "104                 啊，不好意思麻煩你了。讓我擦一下。雖然說實在的髒髒的也沒關係啦...  \n",
       "105  誒，小朋友開始扛神轎的時候，誒...一開始是輪到石井休息。小朋友往前一點之後就換舞獅上場。這...  \n",
       "106                                 可是你不現在吃的話，就不好吃了...  \n",
       "107                          還有啊，也千萬不要再搭公車的時候說有關教授的話題。  \n",
       "\n",
       "[108 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labeled_table_paths = ['JIWC_diff_reason_table.csv', 'CLIWC_diff_reason_table.csv']\n",
    "\n",
    "# label_orientation_list = [\"direct\",\"intense\",\"intense\",\"intense\",\"perspective\"]\n",
    "# intense_orientation_list =['','all','downgrader','specific','']\n",
    "\n",
    "ja_sig_list=[   ['del','cejc','query','request','Trust'],\n",
    "                ['del','cejc','query','thanksgiving','Trust'],\n",
    "                ['del','cejc','res','request','Trust'],\n",
    "                ['add','mpdd','query','apology','Disgust'],\n",
    "                ['add','mpdd','query','request','Sadness'], \n",
    "                ['add','mpdd','query','request','Disgust'],\n",
    "                ['add','mpdd','query','request','Joy'],\n",
    "                ['add','mpdd','query','thanksgiving','Sadness'],\n",
    "                ['add','mpdd','query','thanksgiving','Disgust'],\n",
    "                ['add','mpdd','query','thanksgiving','Trust'], \n",
    "                ['add','mpdd','query','thanksgiving','Joy'],\n",
    "                ['add','mpdd','res','request','Sadness'],\n",
    "                ['add','mpdd','res','request','Disgust'],\n",
    "                ['add','mpdd','res','request','Trust'],\n",
    "                ['add','mpdd','res','request','Joy'],\n",
    "                ['add','mpdd','res','thanksgiving','Sadness']]\n",
    "zh_sig_list=[   ['del',\t'mpdd',\t'query',\t'request',\t\t'affect'],\n",
    "                ['del',\t'mpdd',\t'query',\t'request',\t\t'negemo'],\n",
    "                ['del',\t'mpdd',\t'query',\t'request',\t\t'anger'],\n",
    "                ['del',\t'mpdd',\t'res',\t'thanksgiving',\t'affect'],\n",
    "                ['add',\t'cejc',\t'query',\t'apology',\t    'affect'],\n",
    "                ['add',\t'cejc',\t'query',\t'apology',\t    'posemo'],\n",
    "                ['add',\t'cejc',\t'query',\t'apology',\t    'negemo'],\n",
    "                ['add',\t'cejc',\t'query',\t'apology',\t    'anger'],\n",
    "                ['add',\t'cejc',\t'query',\t'request',\t    'negemo'],\n",
    "                ['add',\t'cejc',\t'res',\t'request',\t    'affect'],\n",
    "                ['add',\t'cejc',\t'res',\t'request',\t    'posemo']]\n",
    "\n",
    "MT_data_list,HT_data_list,prefix_list,columns_list = [],[],[],[]\n",
    "for labeled_table_path in labeled_table_paths:\n",
    "    columns_name=['diff_type','corpus','situation','sen_type','emotion','word','htmt','line','part','effect','direct','intense','perspective']\n",
    "    df = pd.read_csv(f'/nfs/nas-7.1/yamashita/LAB/giza-pp/sentiment_analysis/{labeled_table_path}', names=columns_name)\n",
    "    \n",
    "    if label_orientation == \"intense\" and intense_orientation == \"all\":\n",
    "        more =   ['lessdowngrader','moreupgrader','morespecific','lessrespectful','lesshumble','add_expect_sth_in_return','add_irony']\n",
    "        less = ['moredowngrader','lessspecific','lessupgrader','morerespectful','morehumble','rmv_expect_sth_in_return','rmv_irony']\n",
    "        for m, l in zip(more, less):\n",
    "            df=df.replace(m,'moreintense')\n",
    "            df=df.replace(l,'lessintense')\n",
    "    elif label_orientation == \"intense\":\n",
    "        pass\n",
    "    \n",
    "    if labeled_table_path == 'JIWC_diff_reason_table.csv':\n",
    "        sig_list = ja_sig_list\n",
    "    else:\n",
    "        sig_list = zh_sig_list\n",
    "        \n",
    "    for s in sig_list:\n",
    "        diff_type=s[0]\n",
    "        corpus=s[1]\n",
    "        sen_type=s[2]\n",
    "        situation=s[3]\n",
    "        emotion=s[4]\n",
    "        # FILTER TABLE\n",
    "        df = df.dropna(subset=[label_orientation])\n",
    "        emo_cond = df['diff_type'].isin([diff_type]) & df['corpus'].isin([corpus]) & df['sen_type'].isin([sen_type]) & df['situation'].isin([situation]) & df['emotion'].isin([emotion]) & df['htmt'].isin(['HT'])\n",
    "        gizamiss_cond = df['part'].isin(['gizamiss','labelmiss'])\n",
    "        line_list = df[emo_cond&~gizamiss_cond]['line'].to_list()\n",
    "        label_list = df[emo_cond&~gizamiss_cond][label_orientation].to_list()\n",
    "#         print(label_list)\n",
    "        # GET DATA\n",
    "        MT_path = f'/nfs/nas-7.1/yamashita/LAB/giza-pp/data/{corpus}/{situation}/translated_{sen_type}.csv'\n",
    "        HT_path = f'/nfs/nas-7.1/yamashita/LAB/giza-pp/data/{corpus}/{situation}/rewrited_{sen_type}.csv'\n",
    "        MT_data = get_data_as_list(MT_path)\n",
    "        HT_data = get_data_as_list(HT_path)\n",
    "\n",
    "        for line,label in zip(line_list,label_list):\n",
    "            MT_data_list.append(MT_data[line])\n",
    "            HT_data_list.append(HT_data[line])\n",
    "            prefix_list.append(label)\n",
    "            columns_list.append(MT_path[40:])\n",
    "# print(MT_data_list)\n",
    "tmp_df = pd.DataFrame([prefix_list,MT_data_list,HT_data_list],index=['prefix','input_text','target_text'],columns=columns_list)\n",
    "tmp_df = tmp_df.T\n",
    "data_df = tmp_df.drop_duplicates() \n",
    "data_df = data_df.reset_index().set_axis(['fname','prefix','input_text','target_text',],axis=1)\n",
    "display(data_df)\n",
    "pureidx = np.arange(len(data_df))\n",
    "val_idx = pureidx[5::10]\n",
    "test_idx = pureidx[::10]\n",
    "\n",
    "ind = np.ones(len(data_df), dtype=bool)\n",
    "ind[val_idx] = False\n",
    "ind[test_idx] = False\n",
    "train_idx = pureidx[ind]\n",
    "# print(len(data_df))\n",
    "# print(train_idx.shape)\n",
    "# print(test_idx.shape)\n",
    "# print(val_idx.shape)\n",
    "\n",
    "train_df = data_df.iloc[train_idx]\n",
    "val_df = data_df.iloc[val_idx]\n",
    "test_df = data_df.iloc[test_idx]\n",
    "\n",
    "train_df.to_csv(save_dir+'train.csv', encoding='utf_8_sig')\n",
    "val_df.to_csv(save_dir+'val.csv', encoding='utf_8_sig')\n",
    "test_df.to_csv(save_dir+'test.csv', encoding='utf_8_sig')\n",
    "# display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fname</th>\n",
       "      <th>prefix</th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>mpdd/apology/translated_query.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>謝るのは当然のことです。 気持ちはわかるけど、...... 気持ちは2人の人事です、わかりますか？</td>\n",
       "      <td>謝るのは私の方だよ。そういう風に思わてるって知ってたんだから。でも、そういうのは二人のことだ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>mpdd/request/translated_query.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>ジジュン、もう少し静かにしてくれないか？ 早朝のお喋りしか聞こえてこない! こんなに騒いでた...</td>\n",
       "      <td>静かにしてくれ。朝からギャーギャー言わないでくれ。こんなうるさい女を嫁にしたい男がいると思うか？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>mpdd/request/translated_res.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>情熱の瞬間に人を傷つけたいのか？ 鶯と魏が今どれだけ動揺して嫌われているか知っているのか？</td>\n",
       "      <td>切羽詰まったら何してもいいってこと？ どれだけ二人がつらい思いしたか想像できる？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>mpdd/request/translated_res.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>皆さん、本当にごめんなさい! カップルはここの結婚式場が綺麗でロマンチックだとは思っていませ...</td>\n",
       "      <td>特に出来の良い子が、いろいろと事情があって今日まで結婚式を挙げられなかったのですが……。はい...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45</td>\n",
       "      <td>mpdd/request/translated_res.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>爺さんが話す必要はない、俺はマジュンと白鳩に話しかけてくる。 彼らの家族は林野局の出身者です...</td>\n",
       "      <td>工場長じゃなくて、馬軍、白鴿に言えばいいよ。みんな林業局の一家だし、聴いてもらえるんじゃないかな。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>55</td>\n",
       "      <td>mpdd/request/translated_query.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>先生のお母様、生徒一人一人の親として、対等であるべきです。 クラスの楊貴妃が劉延を追いかけて...</td>\n",
       "      <td>私たちみんなあなたの学生ですよね。楊さんは劉さんが好きみたいだけど、上手くいっていないらしい...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>65</td>\n",
       "      <td>mpdd/request/translated_query.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>今日、私、陳志明は、ここにいる皆さんに証言を求めます、私は、私のガールフレンドである張愛との...</td>\n",
       "      <td>誓いの言葉を聴いてください。私、陳子明は、恋人の張愛を一生大事にします。絶対に傷つけませんし...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>75</td>\n",
       "      <td>mpdd/request/translated_query.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>そう願いましょう。 もちろんシャオドンは下手くそではないし、スッピンなので、こんなゴタゴタを...</td>\n",
       "      <td>そうですね。冬さんは仕事もできるし、根性もあるし、この仕事を十分こなせるでしょう。良ければ、...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>85</td>\n",
       "      <td>mpdd/thanksgiving/translated_res.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>趙斌、こんなんじゃないよ？ 過去は水に流そう 俺たちはまだ友達だからな</td>\n",
       "      <td>昔のことだろ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>95</td>\n",
       "      <td>mpdd/thanksgiving/translated_res.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>というか、国が古参にチャンスを与えてくれたんだから、それを大事にしないといけないよね。 この...</td>\n",
       "      <td>私もよくわかってないけど、国がチャンスをくれたのに、もったいないと思わない？落ちたっていいじ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>105</td>\n",
       "      <td>cejc/apology/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>And+the+kids+mikoshi+came+out+well+the+first+b...</td>\n",
       "      <td>誒，小朋友開始扛神轎的時候，誒...一開始是輪到石井休息。小朋友往前一點之後就換舞獅上場。這...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                 fname      prefix  \\\n",
       "0           5     mpdd/apology/translated_query.csv  lessdirect   \n",
       "1          15     mpdd/request/translated_query.csv  lessdirect   \n",
       "2          25       mpdd/request/translated_res.csv  lessdirect   \n",
       "3          35       mpdd/request/translated_res.csv  lessdirect   \n",
       "4          45       mpdd/request/translated_res.csv  lessdirect   \n",
       "5          55     mpdd/request/translated_query.csv  lessdirect   \n",
       "6          65     mpdd/request/translated_query.csv  lessdirect   \n",
       "7          75     mpdd/request/translated_query.csv  lessdirect   \n",
       "8          85  mpdd/thanksgiving/translated_res.csv  lessdirect   \n",
       "9          95  mpdd/thanksgiving/translated_res.csv  lessdirect   \n",
       "10        105     cejc/apology/translated_query.csv  moredirect   \n",
       "\n",
       "                                           input_text  \\\n",
       "0   謝るのは当然のことです。 気持ちはわかるけど、...... 気持ちは2人の人事です、わかりますか？   \n",
       "1   ジジュン、もう少し静かにしてくれないか？ 早朝のお喋りしか聞こえてこない! こんなに騒いでた...   \n",
       "2       情熱の瞬間に人を傷つけたいのか？ 鶯と魏が今どれだけ動揺して嫌われているか知っているのか？   \n",
       "3   皆さん、本当にごめんなさい! カップルはここの結婚式場が綺麗でロマンチックだとは思っていませ...   \n",
       "4   爺さんが話す必要はない、俺はマジュンと白鳩に話しかけてくる。 彼らの家族は林野局の出身者です...   \n",
       "5   先生のお母様、生徒一人一人の親として、対等であるべきです。 クラスの楊貴妃が劉延を追いかけて...   \n",
       "6   今日、私、陳志明は、ここにいる皆さんに証言を求めます、私は、私のガールフレンドである張愛との...   \n",
       "7   そう願いましょう。 もちろんシャオドンは下手くそではないし、スッピンなので、こんなゴタゴタを...   \n",
       "8                 趙斌、こんなんじゃないよ？ 過去は水に流そう 俺たちはまだ友達だからな   \n",
       "9   というか、国が古参にチャンスを与えてくれたんだから、それを大事にしないといけないよね。 この...   \n",
       "10  And+the+kids+mikoshi+came+out+well+the+first+b...   \n",
       "\n",
       "                                          target_text  \n",
       "0   謝るのは私の方だよ。そういう風に思わてるって知ってたんだから。でも、そういうのは二人のことだ...  \n",
       "1    静かにしてくれ。朝からギャーギャー言わないでくれ。こんなうるさい女を嫁にしたい男がいると思うか？  \n",
       "2            切羽詰まったら何してもいいってこと？ どれだけ二人がつらい思いしたか想像できる？  \n",
       "3   特に出来の良い子が、いろいろと事情があって今日まで結婚式を挙げられなかったのですが……。はい...  \n",
       "4   工場長じゃなくて、馬軍、白鴿に言えばいいよ。みんな林業局の一家だし、聴いてもらえるんじゃないかな。  \n",
       "5   私たちみんなあなたの学生ですよね。楊さんは劉さんが好きみたいだけど、上手くいっていないらしい...  \n",
       "6   誓いの言葉を聴いてください。私、陳子明は、恋人の張愛を一生大事にします。絶対に傷つけませんし...  \n",
       "7   そうですね。冬さんは仕事もできるし、根性もあるし、この仕事を十分こなせるでしょう。良ければ、...  \n",
       "8                                             昔のことだろ。  \n",
       "9   私もよくわかってないけど、国がチャンスをくれたのに、もったいないと思わない？落ちたっていいじ...  \n",
       "10  誒，小朋友開始扛神轎的時候，誒...一開始是輪到石井休息。小朋友往前一點之後就換舞獅上場。這...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456915b438bb49f5a401b6506fb1cec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=86.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of 🤗 Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n",
      "INFO:simpletransformers.t5.t5_model: Training started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using Adafactor for T5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d4460fe59e40c08e30cb22a30ee846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=10.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_model:   Starting fine-tuning.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnatsukinateyamashita\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.33 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">trim-disco-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/natsukinateyamashita/600_culturize_all_both_lenpenalty20_direct\" target=\"_blank\">https://wandb.ai/natsukinateyamashita/600_culturize_all_both_lenpenalty20_direct</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/natsukinateyamashita/600_culturize_all_both_lenpenalty20_direct/runs/spwwjxi0\" target=\"_blank\">https://wandb.ai/natsukinateyamashita/600_culturize_all_both_lenpenalty20_direct/runs/spwwjxi0</a><br/>\n",
       "                Run data is saved locally in <code>/nfs/nas-7.1/yamashita/LAB/SimpleTransformer/wandb/run-20210705_232143-spwwjxi0</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a45e11a103f4becb28e38ec0b8ea9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 0 of 10', max=22.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/torch/nn/modules/module.py:760: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n",
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/optimization.py:557: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  exp_avg_sq_row.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243f8c09cd2f47e9a02d1aa60580e81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of 🤗 Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8231af144642bda523570d58cea69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 1 of 10', max=22.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3313dbbfd9ba4c7b9113c4c209bcd618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of 🤗 Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da424333109545e58dbb236a8dfc2529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 2 of 10', max=22.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8732151a58458cb046ed70ac2ad903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of 🤗 Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c963c278cfa47419f3b1681dc9968c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 3 of 10', max=22.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6246cb04aef7469fbee2de277019fdec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of 🤗 Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b062bd197d9344fda9de95b743c11c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 4 of 10', max=22.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532209f3e2c1474ba829c4ecb16a4852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of 🤗 Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e3361d061141fb9cc2ae17ec228b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 5 of 10', max=22.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d51cbe652c24ab18a8d171975498ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of 🤗 Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2331f956e42d401ea884e0f5464ac7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 6 of 10', max=22.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dedb5a6d039949fa8ad98b70dd099b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of 🤗 Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ceb0c9bd78434cb91fa44e8bd67815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 7 of 10', max=22.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba8e8918b5e426ebf378281b42ddaf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of 🤗 Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0d7d3e5a0e455abe212a2d5849bd13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 8 of 10', max=22.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a3de137245641b781101b2420d6cd92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of 🤗 Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a99e278e654a3cace9b49e1309de52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 9 of 10', max=22.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_utils: Creating features from dataset file at cache_dir/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b794cc413f42c382fa54113b3e3b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of 🤗 Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.t5.t5_model: Training of outputs/100_culturize_all_both_lenpenalty20/best_model/ model complete. Saved to outputs/600_culturize_all_both_lenpenalty20_direct/ckpt/.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(220,\n",
       " {'global_step': [22, 44, 66, 88, 110, 132, 154, 176, 198, 220],\n",
       "  'eval_loss': [3.103783130645752,\n",
       "   3.1018369992574057,\n",
       "   3.104524294535319,\n",
       "   3.102001428604126,\n",
       "   3.1018239657084146,\n",
       "   3.1066388289133706,\n",
       "   3.105625867843628,\n",
       "   3.107701222101847,\n",
       "   3.109158913294474,\n",
       "   3.107808748881022],\n",
       "  'train_loss': [2.792762279510498,\n",
       "   2.4923272132873535,\n",
       "   4.447074890136719,\n",
       "   1.3848614692687988,\n",
       "   3.189361572265625,\n",
       "   2.800020933151245,\n",
       "   2.674372911453247,\n",
       "   2.827849864959717,\n",
       "   2.412982225418091,\n",
       "   2.2289912700653076]})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import sacrebleu\n",
    "import pandas as pd\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "data_dir = f'data/{ver_name}/'\n",
    "train_df = pd.read_csv(f\"{data_dir}train.csv\").astype(str)\n",
    "eval_df = pd.read_csv(f\"{data_dir}val.csv\").astype(str)\n",
    "# train_df[\"prefix\"] = \"\"\n",
    "# eval_df[\"prefix\"] = \"\"\n",
    "display(eval_df)\n",
    "\n",
    "model_args = T5Args()\n",
    "model_args.length_penalty = 20\n",
    "model_args.max_seq_length = 256\n",
    "model_args.train_batch_size = 4\n",
    "model_args.eval_batch_size = 4\n",
    "model_args.num_train_epochs = 10\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_steps = 500\n",
    "model_args.use_multiprocessing = False\n",
    "model_args.fp16 = False\n",
    "model_args.early_stopping_metric = 'eval_loss'\n",
    "model_args.early_stopping_metric_minimize = True\n",
    "model_args.early_stopping_patience = 3\n",
    "model_args.use_early_stopping = True\n",
    "model_args.save_eval_checkpoints = True\n",
    "model_args.save_eval_checkpoints = False\n",
    "# model_args.learning_rate = 3e-5\n",
    "model_args.learning_rate = 3e-8\n",
    "model_args.best_model_dir = f'outputs/{ver_name}/best_model/'\n",
    "model_args.output_dir = f'outputs/{ver_name}/ckpt/'\n",
    "model_args.save_model_every_epoch = True\n",
    "model_args.save_steps = -1\n",
    "model_args.no_cache = True\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.preprocess_inputs = False\n",
    "model_args.num_return_sequences = 1\n",
    "model_args.wandb_project = ver_name\n",
    "\n",
    "model = T5Model(\"mt5\", f'outputs/100_culturize_all_both_lenpenalty20/best_model/', args=model_args, cuda_device=1)\n",
    "# Train the model\n",
    "os.environ['WANDB_CONSOLE'] = 'off'\n",
    "model.train_model(train_df[['prefix','input_text','target_text']], eval_data=eval_df[['prefix','input_text','target_text']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import sacrebleu\n",
    "import pandas as pd\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "model_args = T5Args()\n",
    "model_args.max_length = 256\n",
    "model_args.length_penalty = 1\n",
    "model_args.num_beams = 10\n",
    "\n",
    "model = T5Model(\"mt5\", f\"outputs/{ver_name}/best_model/\", args=model_args, cuda_device=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fname</th>\n",
       "      <th>prefix</th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>cejc/request/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>這樣的話，+如果我不在現場，耀世賣了，+也許我可以給耀世一些保證金。</td>\n",
       "      <td>如果真的要把工作交給耀西的話...能不能給他好一點的利潤啊？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>mpdd/request/translated_query.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>頼むから父の革命家の顔のためにも 解放してくれよ！ よろしくお願いします！」と言っていました...</td>\n",
       "      <td>父の革命家としての顔を立てて、お願い出来ませんか。お願いいたします。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>mpdd/request/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>李華、礼節をわきまえてください、算数の問題を議論しているんですよ! 何を知ってるんだ！</td>\n",
       "      <td>今むずかしい数学の問題を解いてるんだよ。見ればわかるだろ？ 後にしてくれ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>mpdd/request/translated_res.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>柯さん、外見は軟弱だけど、言葉のキレがすごいですね!</td>\n",
       "      <td>お話するまで、こんなに鋭い方だとは思いませんでした。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>mpdd/request/translated_res.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>主催者変更申請の報告。 これは管理事務所のためのものです。 郭爺に見せればいいんだよ。</td>\n",
       "      <td>「主催者変更の申請レポート」これは管理所の仕事だよ。郭さんに見せてくれ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>mpdd/request/translated_res.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>君は自分の仕事をして 私は急いでいない</td>\n",
       "      <td>お気になさらず。私は急ぎませんから。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>60</td>\n",
       "      <td>mpdd/request/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>これは、あまり便利ではありません。 一つには、正午に事務所で休まなければならないこと、もう一...</td>\n",
       "      <td>それは困ります。私はお昼はここで休みます。それに、男女が同じオフィスにいるのもあれでしょうう...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>70</td>\n",
       "      <td>mpdd/request/translated_query.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>劉さん、あなたの選択に敬意を表します！私の心の中では素晴らしい女性です！今後の幸せな結婚をお...</td>\n",
       "      <td>そうですね。僕が間違っていました。許してください。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>80</td>\n",
       "      <td>mpdd/request/translated_query.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>誰がずっと待ってたんだよｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗ...</td>\n",
       "      <td>何言ってんの？ 奥様に用事があってきただけだから、誤解しないでくれる？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>90</td>\n",
       "      <td>mpdd/thanksgiving/translated_res.csv</td>\n",
       "      <td>lessdirect</td>\n",
       "      <td>あの日のあなたは茨に覆われたハリネズミのようで、今日のあなたはあの日とは別人のようです! 態...</td>\n",
       "      <td>ずいぶん雰囲気が柔らかくなりましたね。もうひとつお願いしてもよろしいですか？ 友人として付き...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100</td>\n",
       "      <td>cejc/apology/translated_query.csv</td>\n",
       "      <td>moredirect</td>\n",
       "      <td>這是什麼？。抱歉，胡椒的事。</td>\n",
       "      <td>這是什麼？啊、是青椒啊...抱歉，我不是很喜歡吃誒。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                 fname      prefix  \\\n",
       "0           0     cejc/request/translated_query.csv  moredirect   \n",
       "1          10     mpdd/request/translated_query.csv  lessdirect   \n",
       "2          20     mpdd/request/translated_query.csv  moredirect   \n",
       "3          30       mpdd/request/translated_res.csv  lessdirect   \n",
       "4          40       mpdd/request/translated_res.csv  moredirect   \n",
       "5          50       mpdd/request/translated_res.csv  lessdirect   \n",
       "6          60     mpdd/request/translated_query.csv  moredirect   \n",
       "7          70     mpdd/request/translated_query.csv  lessdirect   \n",
       "8          80     mpdd/request/translated_query.csv  lessdirect   \n",
       "9          90  mpdd/thanksgiving/translated_res.csv  lessdirect   \n",
       "10        100     cejc/apology/translated_query.csv  moredirect   \n",
       "\n",
       "                                           input_text  \\\n",
       "0                  這樣的話，+如果我不在現場，耀世賣了，+也許我可以給耀世一些保證金。   \n",
       "1   頼むから父の革命家の顔のためにも 解放してくれよ！ よろしくお願いします！」と言っていました...   \n",
       "2         李華、礼節をわきまえてください、算数の問題を議論しているんですよ! 何を知ってるんだ！   \n",
       "3                          柯さん、外見は軟弱だけど、言葉のキレがすごいですね!   \n",
       "4         主催者変更申請の報告。 これは管理事務所のためのものです。 郭爺に見せればいいんだよ。   \n",
       "5                                 君は自分の仕事をして 私は急いでいない   \n",
       "6   これは、あまり便利ではありません。 一つには、正午に事務所で休まなければならないこと、もう一...   \n",
       "7   劉さん、あなたの選択に敬意を表します！私の心の中では素晴らしい女性です！今後の幸せな結婚をお...   \n",
       "8   誰がずっと待ってたんだよｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗｗ...   \n",
       "9   あの日のあなたは茨に覆われたハリネズミのようで、今日のあなたはあの日とは別人のようです! 態...   \n",
       "10                                     這是什麼？。抱歉，胡椒的事。   \n",
       "\n",
       "                                          target_text  \n",
       "0                      如果真的要把工作交給耀西的話...能不能給他好一點的利潤啊？  \n",
       "1                  父の革命家としての顔を立てて、お願い出来ませんか。お願いいたします。  \n",
       "2               今むずかしい数学の問題を解いてるんだよ。見ればわかるだろ？ 後にしてくれ。  \n",
       "3                          お話するまで、こんなに鋭い方だとは思いませんでした。  \n",
       "4                「主催者変更の申請レポート」これは管理所の仕事だよ。郭さんに見せてくれ。  \n",
       "5                                  お気になさらず。私は急ぎませんから。  \n",
       "6   それは困ります。私はお昼はここで休みます。それに、男女が同じオフィスにいるのもあれでしょうう...  \n",
       "7                           そうですね。僕が間違っていました。許してください。  \n",
       "8                 何言ってんの？ 奥様に用事があってきただけだから、誤解しないでくれる？  \n",
       "9   ずいぶん雰囲気が柔らかくなりましたね。もうひとつお願いしてもよろしいですか？ 友人として付き...  \n",
       "10                         這是什麼？啊、是青椒啊...抱歉，我不是很喜歡吃誒。  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "eval_df = pd.read_csv(f\"{data_dir}test.csv\").astype(str)\n",
    "display(eval_df)\n",
    "to_ja_truth = [eval_df.loc[eval_df[\"fname\"].str.contains(\"mpdd\")][\"target_text\"].tolist()]\n",
    "to_ja_input = eval_df.loc[eval_df[\"fname\"].str.contains(\"mpdd\")][\"input_text\"].tolist()\n",
    "\n",
    "to_zh_truth = [eval_df.loc[eval_df[\"fname\"].str.contains(\"cejc\")][\"target_text\"].tolist()]\n",
    "to_zh_input = eval_df.loc[eval_df[\"fname\"].str.contains(\"cejc\")][\"input_text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6b01d4e0c74dc7b3327654c5b0ac1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Generating outputs', max=2.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/envs/st/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3216: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of 🤗 Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196150add4cd4deab0bf9b2e790e5ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Decoding outputs', max=9.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------\n",
      "to_ja_bleu:  4.826217438701122\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a92606b10d47c09c49ff14f65c649c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Generating outputs', max=1.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48728e3509e744f982f4515142c71544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Decoding outputs', max=2.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------\n",
      "to_zh_bleu:  2.3040887376159365\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "to_ja_preds = model.predict(to_ja_input)\n",
    "to_ja_bleu = sacrebleu.corpus_bleu(to_ja_preds, to_ja_truth)\n",
    "print(\"--------------------------\")\n",
    "print(\"to_ja_bleu: \", to_ja_bleu.score)\n",
    "\n",
    "to_zh_preds = model.predict(to_zh_input)\n",
    "\n",
    "to_zh_bleu = sacrebleu.corpus_bleu(to_zh_preds, to_zh_truth)\n",
    "print(\"--------------------------\")\n",
    "print(\"to_zh_bleu: \", to_zh_bleu.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_ja_preds.extend(to_zh_preds)\n",
    "to_ja_truth_ = to_ja_truth[0]\n",
    "to_ja_truth_.extend(to_zh_truth[0])\n",
    "\n",
    "r_df = pd.DataFrame([to_ja_preds,to_ja_truth_],index=[f'{ver_name}_preds', 'truth'])\n",
    "r_df.T.to_csv(f'outputs/{ver_name}/preds_truth.csv',encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_df= pd.DataFrame([to_ja_bleu.score,to_zh_bleu.score], index=['to_ja_bleu.score','to_zh_bleu.score'])\n",
    "blue_df.to_csv(f'outputs/{ver_name}/bluescore.csv',encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
