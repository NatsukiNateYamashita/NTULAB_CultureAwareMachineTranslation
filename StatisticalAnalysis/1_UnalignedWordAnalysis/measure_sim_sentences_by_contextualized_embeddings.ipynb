{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import csv\n",
    "import math\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from transformers import BertTokenizer, BertModel ,AdamW\n",
    "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for get SenVec of Original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(f_path):\n",
    "    text_data = []\n",
    "    with open(f_path, 'r', encoding='utf_8_sig') as f:\n",
    "        for line in f:\n",
    "            text_data.append(line[:-1])\n",
    "#             text_data.append(line[:-1].split(' '))\n",
    "    return text_data\n",
    "\n",
    "def encode(tokenizer, data):\n",
    "    \n",
    "    input_ids=[]\n",
    "    attention_mask=[]\n",
    "    token_type_ids=[]\n",
    "    for d in data:\n",
    "        encoded = tokenizer.encode_plus(d,\n",
    "                                    max_length = 128,\n",
    "                                    truncation = True,\n",
    "                                    padding = 'max_length',\n",
    "                                    add_special_tokens = True,\n",
    "                                    pad_to_max_length=True,\n",
    "                                    return_attention_mask=True\n",
    "                                    )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_mask.append(encoded['attention_mask'])\n",
    "        token_type_ids.append(encoded['token_type_ids'])\n",
    "    input_ids= torch.LongTensor(input_ids)\n",
    "    attention_mask= torch.LongTensor(attention_mask)\n",
    "    token_type_ids= torch.LongTensor(token_type_ids)\n",
    "    return input_ids, attention_mask, token_type_ids \n",
    "\n",
    "def on_gpu(input_ids, attention_mask, token_type_ids, device):\n",
    "    if torch.cuda.is_available():\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "    return input_ids, attention_mask, token_type_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for get SenVec of Generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_as_list(fpath) -> list:\n",
    "    data = []\n",
    "    with open(fpath, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        for line in f:\n",
    "            line = line.replace(\"\\n\",\"\").split(\" \")\n",
    "            data.append(line)\n",
    "    return data\n",
    "\n",
    "def get_diff_sentences(df,corpus=\"mpdd\", situation='apology', sen_type='query', diff_type='del', percentile=75, top=True, thirdcol_obj='primitive form'):\n",
    "    def count(x):\n",
    "        return x.count() \n",
    "    df = df[df['corpus'].isin([corpus])&df['situation'].isin([situation])&df['sentence type'].isin([sen_type])&df['difference type'].isin([diff_type])]\n",
    "    unaligned_count = df.groupby(['corpus', 'situation', 'method', 'sentence type', 'difference type','line'])['index'].apply(count).reset_index()\n",
    "    unaligned_count = unaligned_count.rename(columns={'index': 'count'})\n",
    "    \n",
    "#     unaligned_count['quantile'] = unaligned_count.groupby(['corpus', 'situation', 'method', 'sentence type', 'difference type',])['count'].transform(lambda group: np.quantile(group,quantile))\n",
    "#     print(unaligned_count)\n",
    "#     if top==True:\n",
    "#         diff_df = unaligned_count.groupby(['corpus', 'situation', 'method', 'sentence type', 'difference type','line']).filter(lambda group: group['count']>=group['quantile']).reset_index()\n",
    "#     else:\n",
    "#         diff_df = unaligned_count.groupby(['corpus', 'situation', 'method', 'sentence type', 'difference type','line']).filter(lambda group: group['count']<=group['quantile']).reset_index()\n",
    "\n",
    "    unaligned_count.set_index('line',inplace=True)\n",
    "    translated_dict = unaligned_count[unaligned_count['method']=='translated']['count'].to_dict()\n",
    "    rewrited_dict = unaligned_count[unaligned_count['method']=='rewrited']['count'].to_dict()\n",
    "    calced_dict={}\n",
    "    for i in range(max(max(translated_dict),max(rewrited_dict))+1):\n",
    "        if (i in translated_dict) and (i in rewrited_dict):\n",
    "            calced_dict[i] = rewrited_dict[i] - translated_dict[i]\n",
    "        elif (i not in translated_dict) and (i in rewrited_dict):\n",
    "            calced_dict[i] = rewrited_dict[i]\n",
    "        elif (i  in translated_dict) and (i not in rewrited_dict):\n",
    "            calced_dict[i] = - translated_dict[i]\n",
    "        else:\n",
    "            pass\n",
    "    values = list(calced_dict.values())\n",
    "    threshold = np.percentile(np.array(values),percentile)\n",
    "    calced_dict = dict(sorted(calced_dict.items(), key=lambda i: i[1], reverse=True))\n",
    "#     print(calced_dict)\n",
    "#     method_type = ['original','translated','rewrited']\n",
    "    method_type = ['translated','rewrited']\n",
    "#     sentence_type = ['query', 'res']\n",
    "    sentence_type = [sen_type]\n",
    "#     dirs = ['../mrphdata/']\n",
    "    dirs = ['/nfs/nas-7.1/yamashita/LAB/BertRuber/data']\n",
    "    print(f'{corpus} {situation} {diff_type}')\n",
    "    translated = []\n",
    "    rewrited = []\n",
    "    for line, val in calced_dict.items():\n",
    "        if top==True and val < threshold:\n",
    "            continue\n",
    "        if top==False and val > threshold:\n",
    "            continue\n",
    "        for t in sentence_type:\n",
    "            for m in method_type:\n",
    "                for d in dirs:\n",
    "                    path =f'{d}/{corpus}/{situation}/{m}_{t}.csv'\n",
    "                    with open(path, 'r', encoding='utf_8_sig')as f:\n",
    "                        for i,sen in enumerate(f):\n",
    "                            if i == line:\n",
    "                                if m=='translated':\n",
    "                                    translated.append(sen[:-1])\n",
    "                                elif m == 'rewrited':\n",
    "                                    rewrited.append(sen[:-1])\n",
    "#                                     print(sen)\n",
    "#                                 print('[{0:^5}] {1:<11} {2:<6}: {3}'.format(i,m,t,sen[:-1]))\n",
    "#         unaligned_tr = df[(df['line'].isin([line]))&(df['method'].isin(['translated']))]['word'].to_list()\n",
    "#         unaligned_re = df[(df['line'].isin([line]))&(df['method'].isin(['rewrited']))]['word'].to_list()\n",
    "# #             print(type(df))\n",
    "#         print('{0:<11}: {1}'.format('translated',' '.join(unaligned_tr)))\n",
    "#         print('{0:<11}: {1}'.format('rewrited',' '.join(unaligned_re)))\n",
    "#         print()\n",
    "    return translated, rewrited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(groundtruth, generated):\n",
    "    try:\n",
    "        sim = np.dot(groundtruth, generated) / (np.linalg.norm(groundtruth) * np.linalg.norm(generated))\n",
    "    except ZeroDivisionError:\n",
    "        sim = 0.0\n",
    "    return sim\n",
    "def calc_score(original, translated):\n",
    "    sim = np.zeros((translated.shape[0],original.shape[0]))\n",
    "    for i, t in enumerate(translated):\n",
    "        for j, o in enumerate(original):\n",
    "            sim[i][j] = cos_similarity(t,o)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Simplifing Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2senvec(tokenizer,model,device,data):\n",
    "    input_ids, attention_mask, token_type_ids  = encode(tokenizer,data)\n",
    "#     print(input_ids)\n",
    "    # input_tokens = tokenizer.convert_ids_to_tokens([x for x in input_ids[0]])\n",
    "    # print(input_tokens)\n",
    "    input_ids, attention_mask, token_type_ids = on_gpu(input_ids, attention_mask, token_type_ids, device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids,attention_mask,token_type_ids,return_dict=False)\n",
    "    last_hidden_states = outputs[0]\n",
    "#     print(last_hidden_states.size())\n",
    "    sentencevec = last_hidden_states[:,0,:]\n",
    "#     print(sentencevec.size())\n",
    "    sentencevec=sentencevec.cpu().detach().numpy()\n",
    "    return sentencevec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for get word-ranking for removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def calc_tfidf(data):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    x = tfidf.fit_transform(data)\n",
    "    df_tfidf = pd.DataFrame(x.toarray(), columns=tfidf.get_feature_names())\n",
    "    return df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_words_by_unalighed_diff_counts(df,corpus='cejc',situation='apology',sen_type='query', diff_type='del', reverse=False, tfidf=False):\n",
    "    # FOR creating words ranking graph and pos rangling graph\n",
    "#     cols = {'primitive form':'word count', 'pos':'pos count'}\n",
    "    df = df[(df['corpus']==corpus)&(df['situation']==situation)&(df['sentence type']==sen_type)&(df['difference type']==diff_type)]\n",
    "    print('tfidf',tfidf) \n",
    "    cols = {'primitive form':'word count'}\n",
    "    for label_col, data_col in cols.items():\n",
    "        if tfidf == True:\n",
    "            df = df.assign(tfidf = list(np.zeros(len(df))))\n",
    "            dirs = ['/nfs/nas-7.1/yamashita/LAB/giza-pp/primitive']\n",
    "            corpora = ['cejc']\n",
    "            situations = ['apology']\n",
    "            method_type = ['rewrited']\n",
    "            sentence_type = ['query']\n",
    "            data = []\n",
    "            for d in dirs:\n",
    "                for c in corpora:\n",
    "                    for s in situations:\n",
    "                        for m in method_type:\n",
    "                            if m != 'original':\n",
    "                                c = 'mpdd' if corpus == 'cejc' else 'mpdd'\n",
    "                            for t in sentence_type:\n",
    "                                path =f'{d}/{c}/{s}/{m}_{t}'\n",
    "                                with open(path, 'r', encoding='utf_8_sig')as f:\n",
    "                                    for i,sen in enumerate(f):\n",
    "                                        line = sen[:-1]\n",
    "                                        data.append(line)\n",
    "            tfidf_df=calc_tfidf(data)\n",
    "            for index, row in df.iterrows():\n",
    "                line= row['line']\n",
    "                prim= row['primitive form']\n",
    "                try:\n",
    "                    df.loc[index, 'tfidf'] = tfidf_df.loc[line, prim]\n",
    "                except:\n",
    "#                     print(prim)\n",
    "                        pass\n",
    "                \n",
    "            df[data_col] = df.groupby(['corpus', 'situation', 'method', 'sentence type', 'difference type',label_col])['tfidf'].transform(np.sum)\n",
    "        else:\n",
    "            print(False)\n",
    "            df[data_col] = df.groupby(['corpus', 'situation', 'method', 'sentence type', 'difference type',label_col])['index'].transform('count')\n",
    "        \n",
    "        df_word = df.drop_duplicates(['corpus', 'situation', 'method', 'sentence type', 'difference type',label_col]).sort_values(by=data_col, ascending=False)\n",
    "        df_word = df_word.dropna(subset=[label_col])\n",
    "\n",
    "        # GET data for 1 graph\n",
    "#         difference_type = ['del','add']\n",
    "        method_type = ['translated','rewrited']\n",
    "        data, labels = [], []\n",
    "        _temp_w, _temp_wv = [], [] \n",
    "        for method in method_type:\n",
    "            _temp_w.append(df_word[df_word['method']==method][label_col])\n",
    "            _temp_wv.append(df_word[df_word['method']==method][data_col])\n",
    "        # PUT data into dictionary, COUNT freq., CALCULATE (rewrited - translated) and SORT them\n",
    "        dic = {}\n",
    "        for i,(wline,vline) in enumerate(zip(_temp_w,_temp_wv)):\n",
    "            for j,(w,v) in enumerate(zip(wline,vline)):\n",
    "                if w in dic:\n",
    "                    dic[w]['each'][i]=v\n",
    "                    v = v*-1 if i==0 else v\n",
    "                    dic[w]['diff']=dic.get(w,dic.get('diff',0))['diff']+v\n",
    "                else:\n",
    "                    tmp = [0,0]\n",
    "                    tmp[i] = v\n",
    "                    v = v*-1 if i==0 else v\n",
    "                    dic.setdefault(w,{'each':tmp,'diff':v})\n",
    "        dic = sorted(dic.items(),key=lambda x:x[1]['diff'],reverse=reverse)\n",
    "#         pprint(dic)\n",
    "        # exit()\n",
    "#         data_w, data_wv = [],[[],[]]\n",
    "#         for key, values in dic:\n",
    "#             data_w.append(key)\n",
    "#             data_wv[0].append(values['each'][0])\n",
    "#             data_wv[1].append(values['each'][1]) \n",
    "        data_w, data_wv = [],[]\n",
    "        for key, values in dic:\n",
    "            data_w.append(key)\n",
    "            data_wv.append(values['diff'])\n",
    "        labels.append(data_w)\n",
    "        data.append(data_wv)\n",
    "        df_ = pd.DataFrame([data_w,data_wv]).T.set_axis(['word','diff count'],axis='columns')\n",
    "        print(df_)\n",
    "        return df_\n",
    "    \n",
    "def prim2mrph(df,df_,corpus,situation,sen_type,diff_type,mincnt):\n",
    "    ranked_words_list = df_[df_['diff count'] >= mincnt]['word'].to_list()\n",
    "#     print(ranked_words_list)\n",
    "#     print(len(ranked_words_list))\n",
    "    df = df[df[\"corpus\"].isin([corpus])&\\\n",
    "            df['situation'].isin([situation])&\\\n",
    "            df[\"difference type\"].isin([diff_type])&\\\n",
    "            df[\"primitive form\"].isin(ranked_words_list)&\\\n",
    "            df['sentence type'].isin([sen_type])]['word']\n",
    "    word_list = df.to_list()\n",
    "    return word_list\n",
    "\n",
    "def get_highdiff_mrphwords(df,corpus,situation,sen_type,diff_type,mincnt,reverse=False,tfidf=False):\n",
    "    if diff_type=='add':\n",
    "        corpus = 'mpdd' if corpus == 'cejc' else 'cejc'\n",
    "    df_ = rank_words_by_unalighed_diff_counts(df,corpus,situation,sen_type,diff_type,reverse,tfidf)\n",
    "    word_list = prim2mrph(df,df_,corpus,situation,sen_type,diff_type,mincnt)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get SenVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_senvecs(corpus,situation,sen_type,diff_type,percentage,top=True,rm_mincount=3,remove=False,tfidf=False):\n",
    "    # SET ENV USING BERT\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if corpus == \"cejc\":\n",
    "        pretrained_model = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "    elif corpus == \"mpdd\":\n",
    "        pretrained_model = 'bert-base-chinese'\n",
    "    else:\n",
    "        pretrained_model = 'bert-base-multilingual-uncased'\n",
    "    print(pretrained_model)\n",
    "    tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n",
    "    model = BertModel.from_pretrained(pretrained_model)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    \n",
    "    # GET ORIGINAL-TEXT AND ANALYSIS-TABLE\n",
    "    data_dir = '../data/'\n",
    "    original_path = '{}/{}/{}/original_{}.csv'.format(data_dir,corpus,situation,sen_type)\n",
    "    data = get_data(original_path)\n",
    "    \n",
    "    t_path = \"analysis_table.csv\"\n",
    "    # t_path = \"analysis_table_upper.csv\"\n",
    "    df = pd.read_csv(t_path)\n",
    "#         print(data)\n",
    "#     print('original[0]:    ',data[0])\n",
    "    sentencevec = convert2senvec(tokenizer,model,device,data)\n",
    "\n",
    "    tgt_corpus='mpdd' if corpus == 'cejc' else 'cejc'\n",
    "    translated, rewrited = get_diff_sentences(df,tgt_corpus,situation,sen_type,diff_type,percentage,top)\n",
    "        \n",
    "    # REMOVE HIGH-DIFF-COUNT-WORDS FROM ORIGINAL-TEXT\n",
    "    if remove == True:\n",
    "        highdiff_mrphwords = get_highdiff_mrphwords(df,corpus,situation,sen_type,diff_type,rm_mincount,remove,tfidf)\n",
    "        if diff_type == 'del':\n",
    "            for j,w in enumerate(highdiff_mrphwords):\n",
    "#                 if j<=8:\n",
    "#                     continue\n",
    "                for i, d in enumerate(translated):\n",
    "                    if w in d:\n",
    "                        translated[i]=d.replace(w,'')\n",
    "        if diff_type == 'add':\n",
    "            for j,w in enumerate(highdiff_mrphwords):\n",
    "                if type(w) == str:\n",
    "                    pass\n",
    "                elif math.isnan(w):\n",
    "                    continue\n",
    "#                 if j<=8:\n",
    "#                     continue\n",
    "                for i, (d,r) in enumerate(zip(data,rewrited)):\n",
    "#                     if w in d:\n",
    "#                         data[i]=d.replace(w,'')\n",
    "                    if w in r:\n",
    "                        rewrited[i]=r.replace(w,'')\n",
    "            \n",
    "    \n",
    "    print('translated[0]: ', translated[0])\n",
    "    print('rewrited[0]:   ', rewrited[0])\n",
    "    t_sentencevec = convert2senvec(tokenizer,model,device,translated)\n",
    "    r_sentencevec = convert2senvec(tokenizer,model,device,rewrited)\n",
    "    return sentencevec,t_sentencevec,r_sentencevec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cl-tohoku/bert-base-japanese-whole-word-masking\n",
      "mpdd apology add\n",
      "translated[0]:  孫校長さん、こんにちは、本当に申し訳ありません、先に帰ります、私が直接お願いしたわけではありません、許してください、もう紫江を出ましたか？\n",
      "rewrited[0]:    もしもし、校長先生ですか？　本当にすみません。お先に失礼いたしました。事前にちゃんと申請せずに休んでしまって本当に申し訳ございません。もう芷江をお出になりましたか？\n",
      "count    34.000000\n",
      "mean      0.013329\n",
      "std       0.032308\n",
      "min      -0.038637\n",
      "25%      -0.008268\n",
      "50%       0.007928\n",
      "75%       0.036039\n",
      "max       0.099160\n",
      "Name: diff, dtype: float64\n",
      "cl-tohoku/bert-base-japanese-whole-word-masking\n",
      "mpdd apology add\n",
      "tfidf True\n",
      "       word diff count\n",
      "0        んだ     2.2192\n",
      "1    ごめんなさい     1.6445\n",
      "2        お前    1.50489\n",
      "3        なる    1.38679\n",
      "4     申し訳ない    1.37623\n",
      "..      ...        ...\n",
      "260      こと   -0.67305\n",
      "261     なれる  -0.678461\n",
      "262      ます  -0.828039\n",
      "263      ない   -0.98651\n",
      "264      する   -1.56429\n",
      "\n",
      "[265 rows x 2 columns]\n",
      "translated[0]:  孫校長さん、こんにちは、本当に申し訳ありません、先に帰ります、私が直接お願いしたわけではありません、許してください、もう紫江を出ましたか？\n",
      "rewrited[0]:    もしもし、先生ですか？　お先に失礼いたしました事前にせずにしまって申し訳ございませ芷江をお出にましたか？\n",
      "count    34.000000\n",
      "mean     -0.030831\n",
      "std       0.049244\n",
      "min      -0.162792\n",
      "25%      -0.047633\n",
      "50%      -0.027963\n",
      "75%      -0.002779\n",
      "max       0.052956\n",
      "Name: diff, dtype: float64\n",
      "\n",
      "cl-tohoku/bert-base-japanese-whole-word-masking\n",
      "mpdd request add\n",
      "translated[0]:  どうぞお入りください。\n",
      "rewrited[0]:    どうぞ。\n",
      "count    124.000000\n",
      "mean       0.006159\n",
      "std        0.056778\n",
      "min       -0.179944\n",
      "25%       -0.010090\n",
      "50%        0.013723\n",
      "75%        0.033699\n",
      "max        0.151087\n",
      "Name: diff, dtype: float64\n",
      "cl-tohoku/bert-base-japanese-whole-word-masking\n",
      "mpdd request add\n",
      "tfidf True\n",
      "      word diff count\n",
      "0       んだ    1.97286\n",
      "1       ない   0.656297\n",
      "2    すみません   0.434317\n",
      "3       早い   0.398948\n",
      "4       もう    0.34835\n",
      "..     ...        ...\n",
      "876     のだ  -0.603888\n",
      "877     こと   -1.02503\n",
      "878     する   -1.45966\n",
      "879     いる   -1.48743\n",
      "880     ます   -1.98625\n",
      "\n",
      "[881 rows x 2 columns]\n",
      "translated[0]:  どうぞお入りください。\n",
      "rewrited[0]:    どうぞ。\n",
      "count    124.000000\n",
      "mean      -0.000636\n",
      "std        0.056940\n",
      "min       -0.179944\n",
      "25%       -0.023134\n",
      "50%        0.001816\n",
      "75%        0.027002\n",
      "max        0.151087\n",
      "Name: diff, dtype: float64\n",
      "\n",
      "cl-tohoku/bert-base-japanese-whole-word-masking\n",
      "mpdd thanksgiving add\n",
      "translated[0]:  \"\"\"かなり良く印刷されています。 でも、まだまだ仕事はありますよ。 こんにちは、黄編集長ですか？ ....... こんにちは。 管理事務所のラオグオです。 ....... お元気ですか？ ....... 私はここでいいわ おっと 稚拙な記事を書いてしまったので、あなたの出版社が親切に掲載してくれないかと思っています。 ....... 黄編集長、優しすぎる。 ....... わかった、送るよ。 ....... もちろん、今号に掲載した方が良いです。 ....... ええ、ちょっと急なんです。 それを待っています。 ....... いいね！ では、黄編集長、よろしくお願いします。 ....... \"\"さようなら\"\" 誇らしげに微笑んで電話を切った。\"\n",
      "rewrited[0]:    かなり良い出来です。 ただ、もうちょっとだけお願いしたいことが……。 すみません、黄編集長ですか？  管理事務所の郭です。 最近いかがですか？ こちらはうまくいっています。　実は、ちょっと記事を書いてみたのですが、お粗末なものですが、そちらで使っていただけないかと思っておりまして。 ....... 編集長、ありがとうございます。 では、お送りしますね。 もちろん、可能であれば今号に掲載していただきたいです。 ....... ええ、ちょっと急いでいるものですから。わかりました！ では、よろしくお願いします。 失礼します。\n",
      "count    73.000000\n",
      "mean      0.014316\n",
      "std       0.033489\n",
      "min      -0.072831\n",
      "25%      -0.002080\n",
      "50%       0.011749\n",
      "75%       0.033331\n",
      "max       0.099748\n",
      "Name: diff, dtype: float64\n",
      "cl-tohoku/bert-base-japanese-whole-word-masking\n",
      "mpdd thanksgiving add\n",
      "tfidf True\n",
      "     word diff count\n",
      "0      せる   0.283887\n",
      "1      先生   0.258553\n",
      "2    くださる  0.0989149\n",
      "3      んだ  0.0256045\n",
      "4      それ          0\n",
      "..    ...        ...\n",
      "390    のだ   -0.37811\n",
      "391    休む  -0.389528\n",
      "392    こと  -0.747281\n",
      "393    ない  -0.816451\n",
      "394    する   -1.27106\n",
      "\n",
      "[395 rows x 2 columns]\n",
      "translated[0]:  \"\"\"かなり良く印刷されています。 でも、まだまだ仕事はありますよ。 こんにちは、黄編集長ですか？ ....... こんにちは。 管理事務所のラオグオです。 ....... お元気ですか？ ....... 私はここでいいわ おっと 稚拙な記事を書いてしまったので、あなたの出版社が親切に掲載してくれないかと思っています。 ....... 黄編集長、優しすぎる。 ....... わかった、送るよ。 ....... もちろん、今号に掲載した方が良いです。 ....... ええ、ちょっと急なんです。 それを待っています。 ....... いいね！ では、黄編集長、よろしくお願いします。 ....... \"\"さようなら\"\" 誇らしげに微笑んで電話を切った。\"\n",
      "rewrited[0]:    かなり良い出来です。 ただ、もうちょっとだけお願いしたいことが……。 すみまん、黄編集長ですか？  管理事務所の郭です。 最近いかがですか？ こちらはうまくいっています。　実は、ちょっと記事を書いてみたのですが、お粗末なものですが、そちらで使っていただけないかと思っておりまして。 ....... 編集長、ありがとうございます。 では、お送りしますね。 もちろん、可能であれば今号に掲載していただきたいです。 ....... ええ、ちょっと急いでいるものですから。わかりました！ では、よろしくお願いします。 失礼します。\n",
      "count    73.000000\n",
      "mean      0.016031\n",
      "std       0.033998\n",
      "min      -0.072831\n",
      "25%      -0.002080\n",
      "50%       0.013230\n",
      "75%       0.036707\n",
      "max       0.099748\n",
      "Name: diff, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus = 'cejc'\n",
    "sen_type = 'query'\n",
    "diff_type = 'add'\n",
    "percentage = 70\n",
    "rm_mincount = 0.2\n",
    "\n",
    "sentencevec,t_sentencevec,r_sentencevec = get_senvecs(corpus,'apology',sen_type,diff_type,percentage,top=True,rm_mincount=rm_mincount,remove=False,tfidf=True)\n",
    "t_sim = calc_score(sentencevec,t_sentencevec)\n",
    "r_sim = calc_score(sentencevec,r_sentencevec)\n",
    "sim_df = pd.DataFrame([np.mean(t_sim,axis=1),np.mean(r_sim,axis=1)]).T.set_axis([\"Machine Translated\",\"Human Translated\"],axis='columns')\n",
    "sim_df['diff'] = sim_df[\"Human Translated\"]-sim_df[\"Machine Translated\"]\n",
    "# print(sim_df)\n",
    "print(sim_df['diff'].describe())\n",
    "\n",
    "sentencevec,t_sentencevec,r_sentencevec = get_senvecs(corpus,'apology',sen_type,diff_type,percentage,top=True,rm_mincount=rm_mincount,remove=True,tfidf=True)\n",
    "t_sim = calc_score(sentencevec,t_sentencevec)\n",
    "r_sim = calc_score(sentencevec,r_sentencevec)\n",
    "sim_df = pd.DataFrame([np.mean(t_sim,axis=1),np.mean(r_sim,axis=1)]).T.set_axis([\"Machine Translated\",\"Human Translated\"],axis='columns')\n",
    "sim_df['diff'] = sim_df[\"Human Translated\"]-sim_df[\"Machine Translated\"]\n",
    "# print(sim_df)\n",
    "print(sim_df['diff'].describe())\n",
    "print()\n",
    "\n",
    "sentencevec,t_sentencevec,r_sentencevec = get_senvecs(corpus,'request',sen_type,diff_type,percentage,top=True,rm_mincount=rm_mincount,remove=False,tfidf=True)\n",
    "t_sim = calc_score(sentencevec,t_sentencevec)\n",
    "r_sim = calc_score(sentencevec,r_sentencevec)\n",
    "sim_df = pd.DataFrame([np.mean(t_sim,axis=1),np.mean(r_sim,axis=1)]).T.set_axis([\"Machine Translated\",\"Human Translated\"],axis='columns')\n",
    "sim_df['diff'] = sim_df[\"Human Translated\"]-sim_df[\"Machine Translated\"]\n",
    "# print(sim_df)\n",
    "print(sim_df['diff'].describe())\n",
    "\n",
    "sentencevec,t_sentencevec,r_sentencevec = get_senvecs(corpus,'request',sen_type,diff_type,percentage,top=True,rm_mincount=rm_mincount,remove=True,tfidf=True)\n",
    "t_sim = calc_score(sentencevec,t_sentencevec)\n",
    "r_sim = calc_score(sentencevec,r_sentencevec)\n",
    "sim_df = pd.DataFrame([np.mean(t_sim,axis=1),np.mean(r_sim,axis=1)]).T.set_axis([\"Machine Translated\",\"Human Translated\"],axis='columns')\n",
    "sim_df['diff'] = sim_df[\"Human Translated\"]-sim_df[\"Machine Translated\"]\n",
    "# print(sim_df)\n",
    "print(sim_df['diff'].describe())\n",
    "print()\n",
    "\n",
    "sentencevec,t_sentencevec,r_sentencevec = get_senvecs(corpus,'thanksgiving',sen_type,diff_type,percentage,top=True,rm_mincount=rm_mincount,remove=False,tfidf=True)\n",
    "t_sim = calc_score(sentencevec,t_sentencevec)\n",
    "r_sim = calc_score(sentencevec,r_sentencevec)\n",
    "sim_df = pd.DataFrame([np.mean(t_sim,axis=1),np.mean(r_sim,axis=1)]).T.set_axis([\"Machine Translated\",\"Human Translated\"],axis='columns')\n",
    "sim_df['diff'] = sim_df[\"Human Translated\"]-sim_df[\"Machine Translated\"]\n",
    "# print(sim_df)\n",
    "print(sim_df['diff'].describe())\n",
    "\n",
    "sentencevec,t_sentencevec,r_sentencevec = get_senvecs(corpus,'thanksgiving',sen_type,diff_type,percentage,top=True,rm_mincount=rm_mincount,remove=True,tfidf=True)\n",
    "t_sim = calc_score(sentencevec,t_sentencevec)\n",
    "r_sim = calc_score(sentencevec,r_sentencevec)\n",
    "sim_df = pd.DataFrame([np.mean(t_sim,axis=1),np.mean(r_sim,axis=1)]).T.set_axis([\"Machine Translated\",\"Human Translated\"],axis='columns')\n",
    "sim_df['diff'] = sim_df[\"Human Translated\"]-sim_df[\"Machine Translated\"]\n",
    "# print(sim_df)\n",
    "print(sim_df['diff'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
