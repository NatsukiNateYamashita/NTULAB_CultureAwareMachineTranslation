{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 21 21:20:35 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  On   | 00000000:01:00.0 Off |                  N/A |\r\n",
      "|  0%   51C    P8    28W / 260W |   2687MiB / 10986MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0     10393      C   /home/yamashita/anaconda3/bin/python        1793MiB |\r\n",
      "|    0     27490      C   ...117bcc-03eb-42d1-9395-529ddb918b2d.json   879MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertJapaneseTokenizer, AutoTokenizer, AutoModelForSequenceClassification \n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import pipeline\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.dropout = 0.5\n",
    "        self.weight_decay=1e-4\n",
    "        self.lr=1e-5\n",
    "        self.epoches = 500\n",
    "        self.grad_clip = 10\n",
    "        self.batch_size = 8\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negsam_shuffle(encoded,lbl):\n",
    "    ids, att, lbl = np.array(encoded['input_ids']), np.array(encoded['attention_mask']), np.array(lbl)\n",
    "\n",
    "    pos_ids, pos_att, pos_lbl = ids[np.where(lbl==1)], att[np.where(lbl==1)], lbl[np.where(lbl==1)]\n",
    "    neg_ids, neg_att, neg_lbl = ids[np.where(lbl!=1)], att[np.where(lbl!=1)], lbl[np.where(lbl!=1)]\n",
    "\n",
    "    n_pos = pos_ids.shape[0]\n",
    "    n_neg = neg_ids.shape[0]\n",
    "\n",
    "    pos_pureidx = np.arange(n_pos)\n",
    "    random.shuffle(pos_pureidx)\n",
    "    pos_ids, pos_att, pos_lbl = pos_ids[pos_pureidx], pos_att[pos_pureidx], pos_lbl[pos_pureidx]\n",
    "\n",
    "    neg_pureidx = np.arange(n_neg)\n",
    "    neg_pureidx = random.sample(list(neg_pureidx), n_pos)\n",
    "    neg_ids, neg_att, neg_lbl = neg_ids[neg_pureidx], neg_att[neg_pureidx], neg_lbl[neg_pureidx]\n",
    "\n",
    "    ids = np.concatenate([pos_ids, neg_ids]).tolist()\n",
    "    att = np.concatenate([pos_att, neg_att]).tolist()\n",
    "    lbl = np.concatenate([pos_lbl, neg_lbl]).tolist()\n",
    "#     print(ids)\n",
    "#     print(att)\n",
    "#     print(lbl)\n",
    "    negsam_shuffled = {'input_ids': ids,'attention_mask': att}\n",
    "    return negsam_shuffled, lbl\n",
    "\n",
    "def train(conf, model, encoded, labels):\n",
    "    model.train()\n",
    "    inputs_ids = encoded['input_ids']\n",
    "    attention_masks = encoded['attention_mask']\n",
    "    inputs_ids = torch.LongTensor(inputs_ids)\n",
    "    attention_masks = torch.LongTensor(attention_masks)\n",
    "    labels = torch.LongTensor(labels)\n",
    "    loss_ = 0\n",
    "    idx = 0\n",
    "    while True:\n",
    "        if idx+conf.batch_size >= inputs_ids.shape[0]:\n",
    "            b_inputs_ids = inputs_ids[idx:]\n",
    "            b_attention_masks = attention_masks[idx:]\n",
    "            b_labels = labels[idx:]\n",
    "        else:\n",
    "            b_inputs_ids = inputs_ids[idx:idx+conf.batch_size]\n",
    "            b_attention_masks = attention_masks[idx:idx+conf.batch_size]\n",
    "            b_labels = labels[idx:idx+conf.batch_size]\n",
    "            \n",
    "        if torch.cuda.is_available():\n",
    "#             print(b_inputs_ids.shape)\n",
    "            b_inputs_ids = b_inputs_ids.to(conf.device)\n",
    "            b_attention_masks = b_attention_masks.to(conf.device)\n",
    "            b_labels = b_labels.to(conf.device)\n",
    "            model.cuda()\n",
    "        else:\n",
    "            print('CUDA IS NOT AVALABLE')\n",
    "        optimizer.zero_grad()  # 一度計算された勾配結果を0にリセット\n",
    "        output = model(input_ids=b_inputs_ids, attention_mask=b_attention_masks, labels=b_labels)\n",
    "        output.loss.backward() \n",
    "        clip_grad_norm_(model.parameters(), conf.grad_clip)\n",
    "        loss_ += output.loss.detach()\n",
    "        torch.cuda.empty_cache()\n",
    "        idx += conf.batch_size\n",
    "        if idx >= inputs_ids.shape[0]:\n",
    "            break\n",
    "    return loss_\n",
    "\n",
    "def val(conf, model, encoded, labels):\n",
    "    model.eval()\n",
    "    inputs_ids = encoded['input_ids']\n",
    "    attention_masks = encoded['attention_mask']\n",
    "    inputs_ids = torch.LongTensor(inputs_ids)\n",
    "    attention_masks = torch.LongTensor(attention_masks)\n",
    "    labels = torch.LongTensor(labels)\n",
    "    logit = torch.FloatTensor()\n",
    "    loss_ = 0\n",
    "    idx = 0\n",
    "    while True:\n",
    "        if idx+conf.batch_size >= inputs_ids.shape[0]:\n",
    "            b_inputs_ids = inputs_ids[idx:]\n",
    "            b_attention_masks = attention_masks[idx:]\n",
    "            b_labels = labels[idx:]\n",
    "        else:\n",
    "            b_inputs_ids = inputs_ids[idx:idx+conf.batch_size]\n",
    "            b_attention_masks = attention_masks[idx:idx+conf.batch_size]\n",
    "            b_labels = labels[idx:idx+conf.batch_size]\n",
    "            \n",
    "        if torch.cuda.is_available():\n",
    "#             print(b_inputs_ids.shape)\n",
    "            b_inputs_ids = b_inputs_ids.to(conf.device)\n",
    "            b_attention_masks = b_attention_masks.to(conf.device)\n",
    "            b_labels = b_labels.to(conf.device)\n",
    "            model.cuda()\n",
    "        else:\n",
    "            print('CUDA IS NOT AVALABLE')\n",
    "        with torch.no_grad(): ###\n",
    "            optimizer.zero_grad()  # 一度計算された勾配結果を0にリセット\n",
    "            output = model(input_ids=b_inputs_ids, attention_mask=b_attention_masks, labels=b_labels)\n",
    "#         output.loss.backward() \n",
    "#         clip_grad_norm_(model.parameters(), conf.grad_clip)\n",
    "#         optimizer.step()\n",
    "        \n",
    "        loss_ += output.loss.detach()\n",
    "        logit = torch.cat((logit, output.logits.detach().cpu()), 0)\n",
    "        torch.cuda.empty_cache()\n",
    "        idx += conf.batch_size\n",
    "        if idx >= inputs_ids.shape[0]:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    \n",
    "#     inputs_ids = encoded['input_ids']\n",
    "#     attention_masks = encoded['attention_mask']\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():  \n",
    "#         if torch.cuda.is_available():\n",
    "#             inputs_ids = torch.LongTensor(inputs_ids).to(conf.device)\n",
    "#             attention_masks = torch.LongTensor(attention_masks).to(conf.device)\n",
    "#             labels = torch.LongTensor(labels).to(conf.device)\n",
    "#             model.cuda()\n",
    "#             loss, logit = model(input_ids=inputs_ids, attention_mask=attention_masks, labels=labels)\n",
    "#         else:\n",
    "#             print('CUDA IS NOT AVALABLE')\n",
    "    return loss_, logit\n",
    "\n",
    "def compute_metrics(pred, labels):\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbff654a117949bbbf3f38147f752a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=479.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6873edc22bcc459797a704bc0a96d994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=445021143.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b322ca4642547eca5c81444cdf9e5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=257706.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a1deaec44a4bdb90d83dbad2033977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=110.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train_loss: 221.32713317871094  , val_loss: 32.43389892578125   , acc: 0.7293868921775899  , precision: 0.391304347826087   , recall: 0.07317073170731707 , f1: 0.1232876712328767  , patience: 0\n",
      "train_loss: 203.6045684814453   , val_loss: 20.645946502685547  , acc: 0.864693446088795   , precision: 0.9154929577464789  , recall: 0.5284552845528455  , f1: 0.6701030927835052  , patience: 0\n",
      "train_loss: 204.27496337890625  , val_loss: 8.362760543823242   , acc: 0.9682875264270613  , precision: 0.9736842105263158  , recall: 0.9024390243902439  , f1: 0.9367088607594938  , patience: 0\n",
      "train_loss: 225.0279541015625   , val_loss: 3.3167600631713867  , acc: 0.9830866807610994  , precision: 0.967479674796748   , recall: 0.967479674796748   , f1: 0.967479674796748   , patience: 0\n",
      "train_loss: 276.6372985839844   , val_loss: 2.094297409057617   , acc: 0.9894291754756871  , precision: 0.9758064516129032  , recall: 0.983739837398374   , f1: 0.979757085020243   , patience: 0\n",
      "train_loss: 354.3435974121094   , val_loss: 1.738373041152954   , acc: 0.9873150105708245  , precision: 0.968               , recall: 0.983739837398374   , f1: 0.9758064516129032  , patience: 1\n",
      "train_loss: 373.4284973144531   , val_loss: 1.6223936080932617  , acc: 0.9873150105708245  , precision: 0.968               , recall: 0.983739837398374   , f1: 0.9758064516129032  , patience: 2\n",
      "train_loss: 388.3174743652344   , val_loss: 1.5615812540054321  , acc: 0.9873150105708245  , precision: 0.968               , recall: 0.983739837398374   , f1: 0.9758064516129032  , patience: 3\n",
      "[!] early stop\n"
     ]
    }
   ],
   "source": [
    "# tgt_list = ['ja','zh']\n",
    "# seg_list = ['train','dev','test']\n",
    "conf=Config()\n",
    "tgt = 'ja'\n",
    "corpus = 'Friends'\n",
    "label_orientation = \"posinega\"\n",
    "\n",
    "if tgt == 'ja':\n",
    "#     pretrained_model_name = 'daigo/bert-base-japanese-sentiment'\n",
    "    pretrained_model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name,num_labels=2) \n",
    "    tokenizer = BertJapaneseTokenizer.from_pretrained(pretrained_model_name)\n",
    "elif tgt == 'zh':\n",
    "    pretrained_model_name = 'nghuyong/ernie-1.0'\n",
    "#     ch_pretrained_model_name = 'bert-base-chinese'\n",
    "    # ch_pretrained_model_name = 'techthiyanes/chinese_sentiment' # _ を処理する必要がある\n",
    "    # ch_pretrained_model_name = 'hfl/chinese-bert-wwm-ext'\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name,num_labels=2)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name) \n",
    "    \n",
    "if corpus == 'Friends':\n",
    "    if label_orientation == \"posinega\":\n",
    "        negative_labels = ['surprise','fear','anger','disgust','sadness']\n",
    "        positive_labels = 'joy'\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "optimizer = optim.AdamW(model.parameters(), lr=conf.lr\n",
    "                                            # weight_decay=conf.weight_decay,\n",
    "                                            )\n",
    "train_path = f'/nfs/nas-7.1/yamashita/LAB/giza-pp/sentiment_analysis/EmotionLines/{corpus}_{tgt}/friends_train'\n",
    "val_path = f'/nfs/nas-7.1/yamashita/LAB/giza-pp/sentiment_analysis/EmotionLines/{corpus}_{tgt}/friends_dev'\n",
    "\n",
    "columns_name = ['dialogue id','utterance id','speaker','text','emotion','annotation']\n",
    "train_data = pd.read_csv(train_path, names=columns_name)\n",
    "val_data = pd.read_csv(val_path, names=columns_name)\n",
    "\n",
    "train_data['emotion'] = train_data['emotion'].replace(negative_labels,0) # negative\n",
    "train_data['emotion'] = train_data['emotion'].replace(positive_labels,1) # positive\n",
    "train_data = train_data[train_data['emotion'].isin([0,1])]\n",
    "val_data['emotion'] = val_data['emotion'].replace(negative_labels,0) # negative\n",
    "val_data['emotion'] = val_data['emotion'].replace(positive_labels,1) # positive\n",
    "val_data = val_data[val_data['emotion'].isin([0,1])]\n",
    "\n",
    "# ##### DROP NAN\n",
    "# train_data.dropna(axis = 0, how ='any', inplace = True)\n",
    "# val_data.dropna(axis = 0, how ='any', inplace = True)\n",
    "\n",
    "X_train = list(train_data['text'])\n",
    "Y_train = list(train_data['emotion'])\n",
    "X_val = list(val_data['text'])\n",
    "Y_val = list(val_data['emotion'])\n",
    "\n",
    "X_train = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "X_val = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n",
    "      \n",
    "os.makedirs(f'ckpt/{corpus}/{tgt}/{label_orientation}/', exist_ok=True)\n",
    "os.makedirs(f'log/{corpus}/{tgt}/{label_orientation}/', exist_ok=True)\n",
    "\n",
    "train_losses, val_losses, log = [],[],[]\n",
    "best_metric = -1\n",
    "min_loss = 999999\n",
    "for epoch in range(conf.epoches+1):\n",
    "# Negative Sampling and Shuffling\n",
    "    X_train_, Y_train_ = negsam_shuffle(X_train, Y_train)\n",
    "# Train\n",
    "    train_loss = train(conf, model, X_train_, Y_train_)\n",
    "# Val\n",
    "    val_loss, preds = val(conf, model, X_val, Y_val)\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "#     Y_val = Y_val.numpy()\n",
    "    metric_dict = compute_metrics(preds, Y_val)\n",
    "    \n",
    "#     if (best_metric < metric_dict['f1']) or (min_loss > val_loss):\n",
    "    if (best_metric < metric_dict['f1']):\n",
    "        patience = 0\n",
    "        best_metric = metric_dict['f1']\n",
    "        min_loss = val_loss\n",
    "    else:\n",
    "        patience += 1\n",
    "    \n",
    "    state = {'model': model.state_dict(), \n",
    "            'optimizer': optimizer.state_dict(), \n",
    "            'epoch': epoch}\n",
    "    if epoch > 3:\n",
    "        torch.save(state,f'ckpt/{corpus}/{tgt}/{label_orientation}/acc_{metric_dict[\"accuracy\"]}_f1_{metric_dict[\"f1\"]}_vloss_{val_loss}_epoch_{epoch}_lr_{conf.lr}_{pretrained_model_name.replace(\"/\",\")(\")}.pt')\n",
    "    if patience > 3:\n",
    "        print(f'[!] early stop')\n",
    "        break\n",
    "        \n",
    "    print('train_loss: {0:<20}, val_loss: {1:<20}, acc: {2:<20}, precision: {3:<20}, recall: {4:<20}, f1: {5:<20}, patience: {6}'.format(train_loss,val_loss,metric_dict['accuracy'],metric_dict['precision'],metric_dict['recall'],metric_dict['f1'],patience))\n",
    "    log.append([train_loss,val_loss,metric_dict['accuracy'],metric_dict['precision'],metric_dict['recall'],metric_dict['f1'],patience])\n",
    "\n",
    "log = pd.DataFrame(log, columns=['train_loss','val_loss','accuracy','precision','recall','f1','patience'])\n",
    "log.to_csv(f'log/{corpus}/{tgt}/{label_orientation}/{pretrained_model_name.replace(\"/\",\")(\")}_{epoch}_{conf.lr}_{conf.batch_size}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
