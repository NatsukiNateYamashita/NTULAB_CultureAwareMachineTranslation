{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 25 18:08:20 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  On   | 00000000:01:00.0 Off |                  N/A |\r\n",
      "|  0%   51C    P2    65W / 260W |    894MiB / 10986MiB |     12%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0     27490      C   ...117bcc-03eb-42d1-9395-529ddb918b2d.json   879MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertJapaneseTokenizer, AutoTokenizer, AutoModelForSequenceClassification \n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import pipeline\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class Config():\n",
    "    def __init__(self):\n",
    "#         self.dropout = 0.1\n",
    "#         self.weight_decay=1e-4\n",
    "        self.lr=1e-5\n",
    "        self.epoches = 500\n",
    "        self.grad_clip = 10\n",
    "        self.batch_size = 8\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ver_dir = 'culdiff_save/001_add_token_type_id/'\n",
    "os.makedirs(ver_dir, exist_ok=True)\n",
    "\n",
    "conf=Config()\n",
    "# tgt_list = ['ja','zh']\n",
    "tgt = 'ja'\n",
    "# data_diff_type_list = ['del','add','all']\n",
    "data_diff_type = 'all'\n",
    "# label_orientations = [\"direct\",\"intense\",\"perspective\"]\n",
    "label_orientations = [\"direct\"]\n",
    "# intense_orientations =['downgrader','upgrader','specific','respectful','humble','expect_sth_in_return','irony'] \"all\"\n",
    "intense_orientation = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_as_list(path):\n",
    "    data = []\n",
    "    with open(path, 'r', encoding='utf-8-sig')as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    return data\n",
    "\n",
    "def cutout_test_set(encoded,lbl):\n",
    "    ids, att, typ, lbl = np.array(encoded['input_ids']), np.array(encoded['attention_mask']), np.array(encoded['token_type_ids']), np.array(lbl)\n",
    "    pos_ids, pos_att, pos_typ, pos_lbl = ids[np.where(lbl>=1)], att[np.where(lbl>=1)], typ[np.where(lbl>=1)], lbl[np.where(lbl>=1)]\n",
    "    neg_ids, neg_att, neg_typ, neg_lbl = ids[np.where(lbl==0)], att[np.where(lbl==0)], typ[np.where(lbl==0)], lbl[np.where(lbl==0)]\n",
    "\n",
    "    n_pos = pos_ids.shape[0]\n",
    "    n_neg = neg_ids.shape[0]\n",
    "    \n",
    "    # SHUFFLE \n",
    "    pos_pureidx = np.arange(n_pos)\n",
    "    random.shuffle(pos_pureidx)\n",
    "    pos_ids, pos_att, pos_typ, pos_lbl = pos_ids[pos_pureidx], pos_att[pos_pureidx], pos_typ[pos_pureidx], pos_lbl[pos_pureidx]\n",
    "\n",
    "    neg_pureidx = np.arange(n_neg)\n",
    "    random.shuffle(neg_pureidx)\n",
    "#     neg_pureidx = random.sample(list(neg_pureidx), n_pos)\n",
    "    neg_ids, neg_att, neg_typ, neg_lbl = neg_ids[neg_pureidx], neg_att[neg_pureidx], neg_typ[neg_pureidx], neg_lbl[neg_pureidx]\n",
    "    \n",
    "    # CUTOUT TEST\n",
    "    n_test = int(pos_ids.shape[0]/5)\n",
    "    test_pos_ids, test_pos_att, test_pos_typ, test_pos_lbl = pos_ids[:n_test], pos_att[:n_test], pos_typ[:n_test], pos_lbl[:n_test]\n",
    "    test_neg_ids, test_neg_att, test_neg_typ, test_neg_lbl = neg_ids[:n_test], neg_att[:n_test], neg_typ[:n_test], neg_lbl[:n_test]\n",
    "    trainval_pos_ids, trainval_pos_att, trainval_pos_typ, trainval_pos_lbl = pos_ids[n_test:], pos_att[n_test:], pos_typ[n_test:], pos_lbl[n_test:]\n",
    "    trainval_neg_ids, trainval_neg_att, trainval_neg_typ, trainval_neg_lbl = neg_ids[n_test:], neg_att[n_test:], neg_typ[n_test:], neg_lbl[n_test:]\n",
    "    \n",
    "    trainval_ids = np.concatenate([trainval_pos_ids, trainval_neg_ids]).tolist()\n",
    "    trainval_att = np.concatenate([trainval_pos_att, trainval_neg_att]).tolist()\n",
    "    trainval_typ = np.concatenate([trainval_pos_typ, trainval_neg_typ]).tolist()\n",
    "    trainval_lbl = np.concatenate([trainval_pos_lbl, trainval_neg_lbl]).tolist()\n",
    "    test_ids = np.concatenate([test_pos_ids, test_neg_ids]).tolist()\n",
    "    test_att = np.concatenate([test_pos_att, test_neg_att]).tolist()\n",
    "    test_typ = np.concatenate([test_pos_typ, test_neg_typ]).tolist()\n",
    "    test_lbl = np.concatenate([test_pos_lbl, test_neg_lbl]).tolist()\n",
    "    \n",
    "    trainval_encoded = {'input_ids': trainval_ids,'attention_mask': trainval_att,'token_type_ids': trainval_typ}\n",
    "    test_encoded = {'input_ids': test_ids,'attention_mask': test_att,'token_type_ids': test_typ}\n",
    "    \n",
    "    return trainval_encoded, trainval_lbl, test_encoded, test_lbl\n",
    "\n",
    "def make_closs_validation_set(pos_ids, pos_att, pos_typ, pos_lbl, neg_ids, neg_att, neg_typ, neg_lbl):\n",
    "    train_idx = int(pos_ids.shape[0]/5*4)\n",
    "\n",
    "    train_pos_ids, train_pos_att, train_pos_typ, train_pos_lbl, train_neg_ids, train_neg_att, train_neg_typ, train_neg_lbl = pos_ids[:train_idx], pos_att[:train_idx], pos_typ[:train_idx], pos_lbl[:train_idx], neg_ids[:train_idx], neg_att[:train_idx], neg_typ[:train_idx], neg_lbl[:train_idx]\n",
    "    val_pos_ids, val_pos_att, val_pos_typ, val_pos_lbl, val_neg_ids, val_neg_att, val_neg_typ, val_neg_lbl = pos_ids[train_idx:], pos_att[train_idx:], pos_typ[train_idx:], pos_lbl[train_idx:], neg_ids[train_idx:], neg_att[train_idx:], neg_typ[train_idx:], neg_lbl[train_idx:]\n",
    "\n",
    "    train_ids = np.concatenate([train_pos_ids, train_neg_ids]).tolist()\n",
    "    train_att = np.concatenate([train_pos_att, train_neg_att]).tolist()\n",
    "    train_typ = np.concatenate([train_pos_typ, train_neg_typ]).tolist()\n",
    "    train_lbl = np.concatenate([train_pos_lbl, train_neg_lbl]).tolist()\n",
    "    val_ids = np.concatenate([val_pos_ids, val_neg_ids]).tolist()\n",
    "    val_att = np.concatenate([val_pos_att, val_neg_att]).tolist()\n",
    "    val_typ = np.concatenate([val_pos_typ, val_neg_typ]).tolist()\n",
    "    val_lbl = np.concatenate([val_pos_lbl, val_neg_lbl]).tolist()\n",
    "    \n",
    "    return train_ids,train_att,train_typ,train_lbl,val_ids,val_att,val_typ,val_lbl\n",
    "\n",
    "def negsam_shuffle_clossval(encoded,lbl):\n",
    "    ids, att, typ, lbl = np.array(encoded['input_ids']), np.array(encoded['attention_mask']), np.array(encoded['token_type_ids']), np.array(lbl)\n",
    "\n",
    "    pos_ids, pos_att, pos_typ, pos_lbl = ids[np.where(lbl>=1)], att[np.where(lbl>=1)], typ[np.where(lbl>=1)], lbl[np.where(lbl>=1)]\n",
    "    neg_ids, neg_att, neg_typ, neg_lbl = ids[np.where(lbl==0)], att[np.where(lbl==0)], typ[np.where(lbl==0)], lbl[np.where(lbl==0)]\n",
    "\n",
    "    n_pos = pos_ids.shape[0]\n",
    "    n_neg = neg_ids.shape[0]\n",
    "    # SHUFFLE POSI\n",
    "    pos_pureidx = np.arange(n_pos)\n",
    "    random.shuffle(pos_pureidx)\n",
    "    pos_ids, pos_att, pos_typ, pos_lbl = pos_ids[pos_pureidx], pos_att[pos_pureidx], pos_typ[pos_pureidx], pos_lbl[pos_pureidx]\n",
    "    # SHUFFLE & NEGATIVE SAMPLE\n",
    "    neg_pureidx = np.arange(n_neg)\n",
    "    neg_pureidx = random.sample(list(neg_pureidx), n_pos)\n",
    "    neg_ids, neg_att, neg_typ, neg_lbl = neg_ids[neg_pureidx], neg_att[neg_pureidx], neg_typ[neg_pureidx], neg_lbl[neg_pureidx]\n",
    "\n",
    "    # CLOSS VALIDATION      \n",
    "    train_ids,train_att,train_typ,train_lbl,val_ids,val_att,val_typ,val_lbl =  make_closs_validation_set(pos_ids, pos_att, pos_typ, pos_lbl, neg_ids, neg_att, neg_typ, neg_lbl)\n",
    "\n",
    "    train_encoded = {'input_ids': train_ids,'attention_mask': train_att,'token_type_ids': train_typ}\n",
    "    val_encoded = {'input_ids': val_ids,'attention_mask': val_att,'token_type_ids': val_typ}\n",
    "    return train_encoded, train_lbl, val_encoded, val_lbl\n",
    "\n",
    "def train(conf, model, encoded, labels):\n",
    "    model.train()\n",
    "    inputs_ids = encoded['input_ids']\n",
    "    attention_masks = encoded['attention_mask']\n",
    "    token_type_ids = encoded['token_type_ids']\n",
    "    inputs_ids = torch.LongTensor(inputs_ids)\n",
    "    attention_masks = torch.LongTensor(attention_masks)\n",
    "    token_type_ids = torch.LongTensor(token_type_ids)\n",
    "    labels = torch.LongTensor(labels)\n",
    "    loss_ = 0\n",
    "    idx = 0\n",
    "    while True:\n",
    "        if idx+conf.batch_size >= inputs_ids.shape[0]:\n",
    "            b_inputs_ids = inputs_ids[idx:]\n",
    "            b_attention_masks = attention_masks[idx:]\n",
    "            b_token_type_ids = token_type_ids[idx:]\n",
    "            b_labels = labels[idx:]\n",
    "        else:\n",
    "            b_inputs_ids = inputs_ids[idx:idx+conf.batch_size]\n",
    "            b_attention_masks = attention_masks[idx:idx+conf.batch_size]\n",
    "            b_token_type_ids = token_type_ids[idx:idx+conf.batch_size]\n",
    "            b_labels = labels[idx:idx+conf.batch_size]\n",
    "            \n",
    "        if torch.cuda.is_available():\n",
    "#             print(b_inputs_ids.shape)\n",
    "            b_inputs_ids = b_inputs_ids.to(conf.device)\n",
    "            b_attention_masks = b_attention_masks.to(conf.device)\n",
    "            b_token_type_ids = b_token_type_ids.to(conf.device)\n",
    "            b_labels = b_labels.to(conf.device)\n",
    "            model.cuda()\n",
    "        else:\n",
    "            print('CUDA IS NOT AVALABLE')\n",
    "        optimizer.zero_grad()  # 一度計算された勾配結果を0にリセット\n",
    "        output = model(input_ids=b_inputs_ids, attention_mask=b_attention_masks, token_type_ids=b_token_type_ids, labels=b_labels)\n",
    "        output.loss.backward() \n",
    "        clip_grad_norm_(model.parameters(), conf.grad_clip)\n",
    "        loss_ += output.loss.detach()\n",
    "        torch.cuda.empty_cache()\n",
    "        idx += conf.batch_size\n",
    "        if idx >= inputs_ids.shape[0]:\n",
    "            break\n",
    "    return loss_\n",
    "\n",
    "def val(conf, model, encoded, labels):\n",
    "    model.eval()\n",
    "    inputs_ids = encoded['input_ids']\n",
    "    attention_masks = encoded['attention_mask']\n",
    "    token_type_ids = encoded['token_type_ids']\n",
    "    inputs_ids = torch.LongTensor(inputs_ids)\n",
    "    attention_masks = torch.LongTensor(attention_masks)\n",
    "    token_type_ids = torch.LongTensor(token_type_ids)\n",
    "    labels = torch.LongTensor(labels)\n",
    "    logit = torch.FloatTensor()\n",
    "    loss_ = 0\n",
    "    idx = 0\n",
    "    while True:\n",
    "        if idx+conf.batch_size >= inputs_ids.shape[0]:\n",
    "            b_inputs_ids = inputs_ids[idx:]\n",
    "            b_attention_masks = attention_masks[idx:]\n",
    "            b_token_type_ids = token_type_ids[idx:]\n",
    "            b_labels = labels[idx:]\n",
    "        else:\n",
    "            b_inputs_ids = inputs_ids[idx:idx+conf.batch_size]\n",
    "            b_attention_masks = attention_masks[idx:idx+conf.batch_size]\n",
    "            b_token_type_ids = token_type_ids[idx:idx+conf.batch_size]\n",
    "            b_labels = labels[idx:idx+conf.batch_size]\n",
    "        if torch.cuda.is_available():\n",
    "#             print(b_inputs_ids.shape)\n",
    "            b_inputs_ids = b_inputs_ids.to(conf.device)\n",
    "            b_attention_masks = b_attention_masks.to(conf.device)\n",
    "            b_token_type_ids = b_token_type_ids.to(conf.device)\n",
    "            b_labels = b_labels.to(conf.device)\n",
    "            model.cuda()\n",
    "        else:\n",
    "            print('CUDA IS NOT AVALABLE')\n",
    "        with torch.no_grad(): ###\n",
    "            optimizer.zero_grad()  # 一度計算された勾配結果を0にリセット\n",
    "            output = model(input_ids=b_inputs_ids, attention_mask=b_attention_masks, token_type_ids=b_token_type_ids, labels=b_labels)\n",
    "#         output.loss.backward() \n",
    "#         clip_grad_norm_(model.parameters(), conf.grad_clip)\n",
    "#         optimizer.step()\n",
    "        \n",
    "        loss_ += output.loss.detach()\n",
    "        logit = torch.cat((logit, output.logits.detach().cpu()), 0)\n",
    "        torch.cuda.empty_cache()\n",
    "        idx += conf.batch_size\n",
    "        if idx >= inputs_ids.shape[0]:\n",
    "            break\n",
    "\n",
    "    return loss_, logit\n",
    "\n",
    "def compute_metrics(pred, labels):\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    try: # Multi Label\n",
    "        accuracy = accuracy_score(y_true=labels, y_pred=pred, average='weighted')\n",
    "    except: # Bin Label\n",
    "        accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    try: # Multi Label\n",
    "        recall = recall_score(y_true=labels, y_pred=pred, average='weighted')\n",
    "    except: # Bin Label\n",
    "        recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    try: # Multi Label\n",
    "        precision = precision_score(y_true=labels, y_pred=pred, average='weighted')\n",
    "    except: # Bin Label\n",
    "        precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    try: # Multi Label\n",
    "        f1 = f1_score(y_true=labels, y_pred=pred, average='weighted')\n",
    "    except: # Bin Label\n",
    "        f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "#     except: # Bin Label\n",
    "#         accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "#         recall = recall_score(y_true=labels, y_pred=pred)\n",
    "#         precision = precision_score(y_true=labels, y_pred=pred)\n",
    "#         f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for label_orientation in label_orientations:\n",
    "    pass\n",
    "\n",
    "if label_orientation == \"direct\":\n",
    "    num_labels = 3\n",
    "elif label_orientation == \"intense\":\n",
    "    num_labels = 3\n",
    "elif label_orientation == \"perspective\":\n",
    "    num_labels = 5\n",
    "\n",
    "if tgt == 'ja':\n",
    "#     pretrained_model_name = 'daigo/bert-base-japanese-sentiment'\n",
    "    pretrained_model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name,num_labels=num_labels) \n",
    "    tokenizer = BertJapaneseTokenizer.from_pretrained(pretrained_model_name)\n",
    "elif tgt == 'zh':\n",
    "    pretrained_model_name = 'nghuyong/ernie-1.0'\n",
    "#     ch_pretrained_model_name = 'bert-base-chinese'\n",
    "    # ch_pretrained_model_name = 'techthiyanes/chinese_sentiment' # _ を処理する必要がある\n",
    "    # ch_pretrained_model_name = 'hfl/chinese-bert-wwm-ext'\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name,num_labels=num_labels)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name) \n",
    "    \n",
    "optimizer = optim.AdamW(model.parameters(), lr=conf.lr\n",
    "#                                             weight_decay=conf.weight_decay,\n",
    "                                            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET DATA & LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1283\n",
      "1283\n",
      "[['ごめんね！ 私の口調は少し重いです。 王ルオディは単純な女の子なんだから、間に連れてくるのはやめた方がいいよ。 本当は、私に心を寄せるのではなく、良い結果を出すべきなのです。 私のために自分を隠して、自分を大切にしてくれる人との連絡を絶つ必要はないのは言うまでもありません。', '言い方がきつくてごめんね。ただ、王若蝶は単純な子だから、あんまり関わらない方がいいと思う。大事なのは結果を出すことだから、私のことを思ってくれなくてもいいし、私のために他の人とのつながりを断とうとまでは思わないで。'], ['ごめんね！ 誰かにスパイして欲しいと頼みました でも、ホテルで働いてくれって言いに来たわけじゃないんですよ! 会いに行きたかっただけなんですよ！（笑）。 前回出てから思ったんだけど、青城の若い有名ホストや起業家はみんな知ってるよ。 でも、私の記憶の中にあなたのような人はいないわ！ ようやく頭が痛くなってきた！という考えがまだ頭に浮かびませんでした。', 'ごめんなさい。確かに人にあなたを探させたけど、今日来たのはただ顔を見たかっただけなんです。バーで働かせようというのではありません。青城市の名司会者と企業家はみんな知っているけど、あなたみたいな人はいなくて、どうしても思いつかなかったもので……。'], ['すみません！ クオンさんは、あなたの請求書は彼が支払わなければならないと指示しました。 その上、すでに２００００人分の保証金を払っている。', 'すみません。鄺さんから、皆さんの分は絶対に払わせてもらうんだと言われているんです。もう2000元の補償金を出しているのですが……。']]\n",
      "[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "ja_sig_list=[   ['del','cejc','query','request','Trust'],\n",
    "                ['del','cejc','query','thanksgiving','Trust'],\n",
    "                ['del','cejc','res','request','Trust'],\n",
    "                ['add','mpdd','query','apology','Disgust'],\n",
    "                ['add','mpdd','query','request','Sadness'], \n",
    "                ['add','mpdd','query','request','Disgust'],\n",
    "                ['add','mpdd','query','request','Joy'],\n",
    "                ['add','mpdd','query','thanksgiving','Sadness'],\n",
    "                ['add','mpdd','query','thanksgiving','Disgust'],\n",
    "                ['add','mpdd','query','thanksgiving','Trust'], \n",
    "                ['add','mpdd','query','thanksgiving','Joy'],\n",
    "                ['add','mpdd','res','request','Sadness'],\n",
    "                ['add','mpdd','res','request','Disgust'],\n",
    "                ['add','mpdd','res','request','Trust'],\n",
    "                ['add','mpdd','res','request','Joy'],\n",
    "                ['add','mpdd','res','thanksgiving','Sadness']]\n",
    "zh_sig_list=[   ['del',\t'mpdd',\t'query',\t'request',\t\t'affect'],\n",
    "                ['del',\t'mpdd',\t'query',\t'request',\t\t'negemo'],\n",
    "                ['del',\t'mpdd',\t'query',\t'request',\t\t'anger'],\n",
    "                ['del',\t'mpdd',\t'res',\t'thanksgiving',\t'affect'],\n",
    "                ['add',\t'cejc',\t'query',\t'apology',\t    'affect'],\n",
    "                ['add',\t'cejc',\t'query',\t'apology',\t    'posemo'],\n",
    "                ['add',\t'cejc',\t'query',\t'apology',\t    'negemo'],\n",
    "                ['add',\t'cejc',\t'query',\t'apology',\t    'anger'],\n",
    "                ['add',\t'cejc',\t'query',\t'request',\t    'negemo'],\n",
    "                ['add',\t'cejc',\t'res',\t'request',\t    'affect'],\n",
    "                ['add',\t'cejc',\t'res',\t'request',\t    'posemo']]\n",
    "sig_list = []\n",
    "\n",
    "if (tgt=='ja') and (data_diff_type == \"add\"):\n",
    "    for sig in ja_sig_list:\n",
    "        if sig[0] == \"add\":\n",
    "            sig_list.append(sig)\n",
    "    labeled_table_paths = ['JIWC_diff_reason_table.csv']\n",
    "elif (tgt=='ja') and (data_diff_type == \"del\"):\n",
    "    for sig in zh_sig_list:\n",
    "        if sig[0] == \"del\":\n",
    "            sig_list.append(sig)\n",
    "    labeled_table_paths = ['CLIWC_diff_reason_table.csv']\n",
    "elif (tgt=='zh') and (data_diff_type == \"add\"):\n",
    "    for sig in zh_sig_list:\n",
    "        if sig[0] == \"add\":\n",
    "            sig_list.append(sig)\n",
    "    labeled_table_paths = ['CLIWC_diff_reason_table.csv']\n",
    "elif (tgt=='zh') and (data_diff_type == \"del\"):\n",
    "    for sig in ja_sig_list:\n",
    "        if sig[0] == \"del\":\n",
    "            sig_list.append(sig)\n",
    "    labeled_table_paths = ['JIWC_diff_reason_table.csv']\n",
    "elif (tgt=='ja') and (data_diff_type == \"all\"):\n",
    "    for sig in ja_sig_list:\n",
    "        if sig[0] == \"add\":\n",
    "            sig_list.append(sig)\n",
    "    for sig in zh_sig_list:\n",
    "        if sig[0] == \"del\":\n",
    "            sig_list.append(sig)\n",
    "    labeled_table_paths = ['JIWC_diff_reason_table.csv','CLIWC_diff_reason_table.csv']\n",
    "elif (tgt=='zh') and (data_diff_type == \"all\"):\n",
    "    for sig in zh_sig_list:\n",
    "        if sig[0] == \"add\":\n",
    "            sig_list.append(sig)\n",
    "    for sig in ja_sig_list:\n",
    "        if sig[0] == \"del\":\n",
    "            sig_list.append(sig)\n",
    "    labeled_table_paths = ['JIWC_diff_reason_table.csv','CLIWC_diff_reason_table.csv']\n",
    "\n",
    "data_pair,labels=[],[]\n",
    "for labeled_table_path in labeled_table_paths:\n",
    "    columns_name=['diff_type','corpus','situation','sen_type','emotion','word','htmt','line','part','effect','direct','intense','perspective']\n",
    "    df = pd.read_csv(labeled_table_path, names=columns_name)\n",
    "\n",
    "    # REPLACE subordinate concept to MOREorLESS intencity\n",
    "    if label_orientation != \"intense\":\n",
    "        more =   ['lessdowngrader','moreupgrader','morespecific','lessrespectful','lesshumble','add_expect_sth_in_return','add_irony']\n",
    "        less = ['moredowngrader','lessspecific','lessupgrader','morerespectful','morehumble','rmv_expect_sth_in_return','rmv_irony']\n",
    "        for m, l in zip(more, less):\n",
    "            df=df.replace(m,'moreintense')\n",
    "            df=df.replace(l,'lessintense')\n",
    "    elif label_orientation == \"intense\" and intense_orientation == \"all\":\n",
    "        more =   ['lessdowngrader','moreupgrader','morespecific','lessrespectful','lesshumble','add_expect_sth_in_return','add_irony']\n",
    "        less = ['moredowngrader','lessspecific','lessupgrader','morerespectful','morehumble','rmv_expect_sth_in_return','rmv_irony']\n",
    "        for m, l in zip(more, less):\n",
    "            df=df.replace(m,'moreintense')\n",
    "            df=df.replace(l,'lessintense')\n",
    "    elif label_orientation == \"intense\":\n",
    "        pass\n",
    "\n",
    "    # REPLACE labels to ids\n",
    "    if label_orientation == \"direct\":\n",
    "        df = df.replace(f\"more{label_orientation}\",2) # more: 2\n",
    "        df = df.replace(f\"less{label_orientation}\",1) # less: 1 \n",
    "        df.loc[~(df[label_orientation].isin([1,2])), label_orientation]= 0 # other: 0\n",
    "    elif label_orientation == \"intense\" and intense_orientation == \"all\":\n",
    "        df = df.replace(f\"more{label_orientation}\",2) # more: 2\n",
    "        df = df.replace(f\"less{label_orientation}\",1) # less: 1 \n",
    "        df.loc[~(df[label_orientation].isin([1,2])), label_orientation]= 0 # other: 0    \n",
    "    elif label_orientation == \"intense\":\n",
    "        if intense_orientation in ['upgrader','specific',]:\n",
    "            df = df.replace(f\"more{intense_orientation}\",2) # more intense: 2\n",
    "            df = df.replace(f\"less{intense_orientation}\",1) # less intense: 1 \n",
    "        if intense_orientation in ['downgrader','respectful','humble']:\n",
    "            df = df.replace(f\"less{intense_orientation}\",2) # more intense: 1\n",
    "            df = df.replace(f\"more{intense_orientation}\",1) # less intense: 2 \n",
    "        elif intense_orientation in ['expect_sth_in_return','irony']:\n",
    "            df = df.replace(f\"add_{intense_orientation}\",2) # more intense: 2\n",
    "            df = df.replace(f\"rmv_{intense_orientation}\",1) # less intense: 1 \n",
    "        df.loc[~(df[label_orientation].isin([1,2])), label_orientation]= 0 # other: 0   \n",
    "    elif label_orientation == \"perspective\":\n",
    "        df = df.replace(f\"speaker_oriented\",int(4)) \n",
    "        df = df.replace(f\"listener_oriented\",int(3))\n",
    "        df = df.replace(f\"speaker_listener_oriented\",int(2))\n",
    "        df = df.replace(f\"impersonal_oriented\",int(1))\n",
    "        df.loc[~df[label_orientation].isin([1,2,3,4]), label_orientation] = 0 # other: 0\n",
    "\n",
    "    df[label_orientation] = df[label_orientation].astype('int8')\n",
    "    index_names=[]\n",
    "    tmp_data_pair = []\n",
    "    tmp_labels = []\n",
    "    for s in sig_list:\n",
    "        diff_type=s[0]\n",
    "        corpus=s[1]\n",
    "        sen_type=s[2]\n",
    "        situation=s[3]\n",
    "        emotion=s[4]\n",
    "        # FILTER TABLE\n",
    "        emo_cond = df['diff_type'].isin([diff_type]) & df['corpus'].isin([corpus]) & df['sen_type'].isin([sen_type]) & df['situation'].isin([situation]) & df['emotion'].isin([emotion])\n",
    "        gizamiss_cond = df['part'].isin(['gizamiss','labelmiss'])\n",
    "    #     line = df[emo_cond&~gizamiss_cond]['line'].to_list()\n",
    "    #     labels = df[emo_cond&~gizamiss_cond]['line'].to_list()\n",
    "        line_list = df[emo_cond]['line'].to_list()\n",
    "        label_list = df[emo_cond][label_orientation].to_list()\n",
    "    #     word = df[emo_cond&~gizamiss_cond]['word'].to_list()\n",
    "        # GET DATA\n",
    "        if tgt == 'ja':\n",
    "            MT_path = f'../data/mpdd/{situation}/translated_{sen_type}.csv'\n",
    "            HT_path = f'../data/mpdd/{situation}/rewrited_{sen_type}.csv'\n",
    "        elif tgt == 'zh':\n",
    "            MT_path = f'../data/cejc/{situation}/translated_{sen_type}.csv'\n",
    "            HT_path = f'../data/cejc/{situation}/rewrited_{sen_type}.csv'\n",
    "\n",
    "        MT_data = get_data_as_list(MT_path)\n",
    "        HT_data = get_data_as_list(HT_path)\n",
    "    #     MT_unaligned = get_mrphdata_as_list(MT_unaligned_path)\n",
    "    #     HT_unaligned = get_mrphdata_as_list(HT_unaligned_path)\n",
    "    #     MT_mrph = get_mrphdata_as_list(MT_mrph_path)\n",
    "    #     HT_mrph = get_mrphdata_as_list(HT_mrph_path)\n",
    "\n",
    "        for line,label in zip(line_list,label_list):\n",
    "#             if len(MT_data[line][0]) >= 509:\n",
    "#                 MT_data[line][0]=MT_data[line][0][:509]\n",
    "#             if len(HT_data[line][0]) >= 509:\n",
    "#                 HT_data[line][0]=HT_data[line][0][:509]\n",
    "            tmp_data_pair.append([MT_data[line][0],HT_data[line][0]])\n",
    "            tmp_labels.append(label) \n",
    "    data_pair.extend(tmp_data_pair)\n",
    "    labels.extend(tmp_labels)\n",
    "# display(df[df[label_orientation].isin([1,2])])\n",
    "print(len(data_pair))\n",
    "print(len(labels))\n",
    "print(data_pair[:3])\n",
    "print(labels[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of test-set:  62\n",
      "[!] Clear the checkpoints under culdiff_save/001_add_token_type_id/data/ja/all/direct/\n",
      "[!] Clear the checkpoints under culdiff_save/001_add_token_type_id/ckpt/ja/all/direct/\n",
      "[!] Clear the checkpoints under culdiff_save/001_add_token_type_id/log/ja/all/direct/\n"
     ]
    }
   ],
   "source": [
    "X_train = tokenizer(data_pair, padding=True, truncation=True, max_length=512)\n",
    "Y_train = labels\n",
    "######################################################\n",
    "# CUTOUT TRAINVAL-SET/TEST-SET & SAVE THEM \n",
    "X_train, Y_train, test_encoded, test_lbl = cutout_test_set(X_train,Y_train)\n",
    "print('len of test-set: ', len(test_lbl))\n",
    "\n",
    "if label_orientation != 'intense':\n",
    "    save_data_dir = f'{ver_dir}data/{tgt}/{data_diff_type}/{label_orientation}/'\n",
    "    save_ckpt_dir = f'{ver_dir}ckpt/{tgt}/{data_diff_type}/{label_orientation}/'\n",
    "    save_log_dir = f'{ver_dir}log/{tgt}/{data_diff_type}/{label_orientation}/'\n",
    "elif label_orientation == 'intense':\n",
    "    save_data_dir = f'{ver_dir}data/{tgt}/{data_diff_type}/{label_orientation}/{intense_orientation}/'\n",
    "    save_ckpt_dir = f'{ver_dir}ckpt/{tgt}/{data_diff_type}/{label_orientation}/{intense_orientation}/'\n",
    "    save_log_dir = f'{ver_dir}log/{tgt}/{data_diff_type}/{label_orientation}/{intense_orientation}/'\n",
    "    \n",
    "os.makedirs(save_data_dir, exist_ok=True)\n",
    "os.makedirs(save_ckpt_dir, exist_ok=True)\n",
    "os.makedirs(save_log_dir, exist_ok=True)\n",
    "os.system(f'rm {save_data_dir}*')\n",
    "print(f'[!] Clear the checkpoints under {save_data_dir}')\n",
    "os.system(f'rm {save_ckpt_dir}*')\n",
    "print(f'[!] Clear the checkpoints under {save_ckpt_dir}')\n",
    "os.system(f'rm {save_log_dir}*')\n",
    "print(f'[!] Clear the checkpoints under {save_log_dir}')\n",
    "\n",
    "with open(f\"{save_data_dir}trainval_data\", mode=\"wb\") as f:\n",
    "    pickle.dump([X_train, Y_train], f)\n",
    "with open(f\"{save_data_dir}test_data\", mode=\"wb\") as f:\n",
    "    pickle.dump([test_encoded, test_lbl], f)\n",
    "\n",
    "test_data_string = []\n",
    "for t , l in zip(test_encoded['input_ids'],test_lbl):\n",
    "    t = tokenizer.batch_decode(t,skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "    test_data_string.append([t,l])\n",
    "with open(f\"{save_data_dir}test_data.string\", \"w\", encoding=\"utf_8_sig\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(test_data_string)\n",
    "######################################################\n",
    "# # LOAD TRAINVAL-SET/TEST-SET \n",
    "# with open(f\"culdiff_data/{tgt}/{label_orientation}/test_data\", mode=\"rb\") as f:\n",
    "#     X_train, Y_train = pickle.load(f)\n",
    "# with open(f\"culdiff_data/{tgt}/{label_orientation}/test_data\", mode=\"rb\") as f:\n",
    "#     test_encoded, test_lbl = pickle.load(f)\n",
    "######################################################\n",
    "# del test_encoded, test_lbl, test_data_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 27.058481216430664  , val_loss: 7.543033123016357   , acc: 0.46153846153846156 , precision: 0.4341085271317829  , recall: 0.46153846153846156 , f1: 0.3678160919540229  , patience: 0\n",
      "train_loss: 26.908424377441406  , val_loss: 7.50247859954834    , acc: 0.40384615384615385 , precision: 0.23863636363636365 , recall: 0.40384615384615385 , f1: 0.30000000000000004 , patience: 1\n",
      "train_loss: 27.569162368774414  , val_loss: 7.464454650878906   , acc: 0.4807692307692308  , precision: 0.41557555919258043 , recall: 0.4807692307692308  , f1: 0.3631077826283305  , patience: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamashita/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 27.23859977722168   , val_loss: 7.457955837249756   , acc: 0.4807692307692308  , precision: 0.25510204081632654 , recall: 0.4807692307692308  , f1: 0.33333333333333337 , patience: 3\n",
      "train_loss: 27.106962203979492  , val_loss: 7.465885639190674   , acc: 0.5                 , precision: 0.7411858974358976  , recall: 0.5                 , f1: 0.3748200863585479  , patience: 0\n",
      "train_loss: 27.22520637512207   , val_loss: 7.435189247131348   , acc: 0.5                 , precision: 0.2708333333333333  , recall: 0.5                 , f1: 0.35135135135135137 , patience: 1\n",
      "train_loss: 27.35380744934082   , val_loss: 7.500955104827881   , acc: 0.4807692307692308  , precision: 0.2604166666666667  , recall: 0.4807692307692308  , f1: 0.33783783783783783 , patience: 2\n",
      "train_loss: 27.18480682373047   , val_loss: 7.538060665130615   , acc: 0.4230769230769231  , precision: 0.25                , recall: 0.4230769230769231  , f1: 0.3142857142857143  , patience: 3\n",
      "[!] early stop\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, log = [],[],[]\n",
    "best_metric = -1\n",
    "min_loss = 999999\n",
    "print()\n",
    "for epoch in range(conf.epoches+1):\n",
    "# Negative Sampling and Shuffling and Clossvalidaition\n",
    "    X_train_, Y_train_, X_val, Y_val = negsam_shuffle_clossval(X_train, Y_train)\n",
    "# Train\n",
    "    train_loss = train(conf, model, X_train_, Y_train_)\n",
    "# Val\n",
    "    val_loss, preds = val(conf, model, X_val, Y_val)\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "#     Y_val = Y_val.numpy()\n",
    "    metric_dict = compute_metrics(preds, Y_val)\n",
    "#     confusion_matrix(Y_val, preds, labels=[0,1,2])\n",
    "    \n",
    "#     if (best_metric < metric_dict['f1']) or (min_loss > val_loss):\n",
    "    if (best_metric < metric_dict['f1']):\n",
    "        patience = 0\n",
    "        best_metric = metric_dict['f1']\n",
    "        min_loss = val_loss\n",
    "    else:\n",
    "        patience += 1\n",
    "    \n",
    "    state = {'model': model.state_dict(), \n",
    "            'optimizer': optimizer.state_dict(), \n",
    "            'epoch': epoch}\n",
    "#     if epoch > 1:\n",
    "    torch.save(state,f'{save_ckpt_dir}acc_{metric_dict[\"accuracy\"]}_f1_{metric_dict[\"f1\"]}_vloss_{val_loss}_epoch_{epoch}_lr_{conf.lr}_{pretrained_model_name.replace(\"/\",\")(\")}.pt')\n",
    "    if patience > 3:\n",
    "        print(f'[!] early stop')\n",
    "        break\n",
    "        \n",
    "    print('train_loss: {0:<20}, val_loss: {1:<20}, acc: {2:<20}, precision: {3:<20}, recall: {4:<20}, f1: {5:<20}, patience: {6}'.format(train_loss,val_loss,metric_dict['accuracy'],metric_dict['precision'],metric_dict['recall'],metric_dict['f1'],patience))\n",
    "    log.append([train_loss,val_loss,metric_dict['accuracy'],metric_dict['precision'],metric_dict['recall'],metric_dict['f1'],patience])\n",
    "\n",
    "log = pd.DataFrame(log, columns=['train_loss','val_loss','accuracy','precision','recall','f1','patience'])\n",
    "log.to_csv(f'{save_log_dir}{pretrained_model_name.replace(\"/\",\")(\")}_{epoch}_{conf.lr}_{conf.batch_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_best_model(tgt, path, num_labels):\n",
    "    import torch\n",
    "    best_acc, best_f1, best_epoch, best_file, best_pretrained_model_name = -1, -1, -1, None, None\n",
    "    \n",
    "    for file in os.listdir(path):\n",
    "        try:\n",
    "#             /acc_0.5454545454545454_f1_0.4761904761904762_vloss_2.8154916763305664_epoch_4_lr_1e-05_nghuyong)(ernie-1.0.pt\n",
    "            _, acc, _, f1, _, loss, _, epoch, _, lr, pretrained_model_name = file.split(\"_\")\n",
    "            pretrained_model_name = pretrained_model_name[:-3]\n",
    "            pretrained_model_name = pretrained_model_name.replace(\")(\",\"/\")\n",
    "        except:\n",
    "            continue\n",
    "        acc = float(acc)\n",
    "        f1 = float(f1)\n",
    "        epoch = int(epoch)\n",
    "        if f1 > best_f1:\n",
    "            best_file = file\n",
    "            best_epoch = epoch\n",
    "            best_acc = acc\n",
    "            best_f1 = f1\n",
    "            best_loss = loss\n",
    "            best_pretrained_model_name = pretrained_model_name\n",
    "        elif f1 == best_f1:\n",
    "            if best_loss > loss:\n",
    "                best_file = file\n",
    "                best_epoch = epoch\n",
    "                best_acc = acc\n",
    "                best_f1 = f1\n",
    "                best_loss = loss\n",
    "                best_pretrained_model_name = pretrained_model_name               \n",
    "    if best_file:\n",
    "        file_path = path + best_file\n",
    "        if tgt == 'ja':\n",
    "            tokenizer = BertJapaneseTokenizer.from_pretrained(best_pretrained_model_name)\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(best_pretrained_model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(best_pretrained_model_name,num_labels=num_labels)\n",
    "        model.load_state_dict(torch.load(file_path)['model'])\n",
    "        print(f'[!] Load the model from {file_path}')\n",
    "    else:\n",
    "        raise Exception(f\"[!] No saved model\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Load the model from culdiff_save/001_add_token_type_id/ckpt/ja/all/direct/acc_0.5_f1_0.3748200863585479_vloss_7.465885639190674_epoch_4_lr_1e-05_cl-tohoku)(bert-base-japanese-whole-word-masking.pt\n",
      "[!] Clear the checkpoints under culdiff_save/001_add_token_type_id/test/ja/all/direct/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1  2\n",
       "0  28  2  1\n",
       "1  27  0  4\n",
       "2   0  0  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.45161290322580644 , precision: 0.2545454545454545  , recall: 0.45161290322580644 , f1: 0.32558139534883723 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "if label_orientation != 'intense':\n",
    "    test_data_dir = f'{ver_dir}data/{tgt}/{data_diff_type}/{label_orientation}/'\n",
    "    ckpt_dir = f'{ver_dir}ckpt/{tgt}/{data_diff_type}/{label_orientation}/'\n",
    "    test_result_dir = f'{ver_dir}test/{tgt}/{data_diff_type}/{label_orientation}/'\n",
    "elif label_orientation == 'intense':\n",
    "    test_data_dir = f'{ver_dir}data/{tgt}/{data_diff_type}/{label_orientation}/{intense_orientation}/'\n",
    "    ckpt_dir = f'{ver_dir}ckpt/{tgt}/{data_diff_type}/{label_orientation}/{intense_orientation}/'\n",
    "    test_result_dir = f'{ver_dir}test/{tgt}/{data_diff_type}/{label_orientation}/{intense_orientation}/'\n",
    "    \n",
    "model,tokenizer = load_best_model(tgt, ckpt_dir, num_labels)\n",
    "\n",
    "val_loss, preds = val(conf, model, test_encoded, test_lbl)\n",
    "preds = preds.detach().cpu().numpy()\n",
    "metric_dict = compute_metrics(preds, test_lbl)\n",
    "\n",
    "os.makedirs(f'{test_result_dir}', exist_ok=True)\n",
    "os.system(f'rm {test_result_dir}*')\n",
    "print(f'[!] Clear the checkpoints under {test_result_dir}')\n",
    "\n",
    "confusion_matrix_ = confusion_matrix(test_lbl, np.argmax(preds,axis=1), labels=list(range(num_labels)))\n",
    "confusion_matrix_ = pd.DataFrame(confusion_matrix_)\n",
    "confusion_matrix_.to_csv(f'{test_result_dir}confusion_matrix.csv',header=True, index=True)\n",
    "display(confusion_matrix_)\n",
    "\n",
    "preds = pd.DataFrame(preds)\n",
    "preds.to_csv(f'{test_result_dir}logits.csv',header=False, index=False)\n",
    "\n",
    "log = pd.DataFrame([metric_dict['accuracy'],metric_dict['precision'],metric_dict['recall'],metric_dict['f1']], index=['accuracy','precision','recall','f1'])\n",
    "log.to_csv(f'{test_result_dir}metric.csv',header=False, index=True)\n",
    "print('acc: {0:<20}, precision: {1:<20}, recall: {2:<20}, f1: {3:<20}'.format(metric_dict['accuracy'],metric_dict['precision'],metric_dict['recall'],metric_dict['f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
